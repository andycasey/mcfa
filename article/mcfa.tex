\documentclass[twocolumn]{aastex62}
\usepackage{bm}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{color}
\usepackage{comment}
\usepackage{layouts}

\newcommand\teff{T_{\rm eff}}
\newcommand\logg{\log{g}}
\newcommand\feh{[\rm{Fe}/\rm{H}]}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\package}[1]{\texttt{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\article}{\emph{Article}}

\newcommand{\Gaia}{\project{Gaia}}
\newcommand{\gaia}{\project{gaia}}
\newcommand{\Galah}{\project{Galah}}
\newcommand{\GALAH}{\project{GALAH}}
\newcommand{\APOGEE}{\project{APOGEE}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}


\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\renewcommand{\vec}[1]{\vect{#1}}

\newcommand{\weight}{\pi}
\newcommand{\data}{\textbf{Y}}
\newcommand{\vecdata}{\vec\data}
\newcommand{\vecdataunscaled}{\vec{X}}
\newcommand{\diag}[1]{\textrm{diag}(#1)}

\newcommand{\nextstep}{^\textrm{(t+1)}}
\newcommand{\thisstep}{^\textrm{(t)}}
\newcommand{\transpose}{^\intercal}
\newcommand{\eye}{\textbf{I}}

\newcommand{\factorloads}{\textbf{L}}
\newcommand{\factorscores}{\textbf{S}}
\newcommand{\specificvariance}{\vec{D}}

\newcommand{\scoremeans}{\vec\xi}
\newcommand{\scorecovs}{\vec\Omega}

\newcommand{\NumData}{N}
\newcommand{\NumDimensions}{D}
\newcommand{\numdata}{n}
\newcommand{\numdimensions}{d}
\newcommand{\NumLatentFactors}{J}
\newcommand{\numlatentfactors}{j}
\newcommand{\NumComponents}{K}
\newcommand{\numcomponents}{k}
\newcommand{\likelihood}{\mathcal{L}}
\newcommand{\ExperimentHash}{96ff8}



\received{2019}
\revised{2019}
\accepted{2019}

\newcommand{\vcpath}{vc.tex}

\IfFileExists{\vcpath}{\input{\vcpath}}{
	\newcommand{\giturl}{UNKNOWN}
	\newcommand{\gitslug}{UNKNOWN}
	\newcommand{\githash}{UNKNOWN}
	\newcommand{\gitdate}{UNKNOWN}
	\newcommand{\gitauthor}{UNKNOWN}
}



\submitjournal{AAS Journals}

\shorttitle{A data-driven model of nucleosynthesis with chemical tagging in latent space}
\shortauthors{Casey et al.}

\begin{document}

\title{A data-driven model of nucleosynthesis with chemical tagging in latent space}

\correspondingauthor{Andrew R. Casey}
\email{andrew.casey@monash.edu}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\affiliation{School of Physics \& Astronomy, 
			 Monash University,
			 Wellington Rd, Clayton 3800, Victoria, Australia}
\affiliation{Faculty of Information Technology, 
			 Monash University, 
			 Wellington Rd, Clayton 3800, Victoria, Australia}
			 
\author{John Lattanzio}
\affiliation{School of Physics \& Astronomy, 
			 Monash University,
			 Wellington Rd, Clayton 3800, Victoria, Australia}

\author{Aldeida Aleti}
\affiliation{Faculty of Information Technology, 
			 Monash University, 
			 Wellington Rd, Clayton 3800, Victoria, Australia}

\author{David Dowe}
\affiliation{Faculty of Information Technology, 
			 Monash University, 
			 Wellington Rd, Clayton 3800, Victoria, Australia}
			 
\author{the GALAH team}


\begin{abstract}
Chemical tagging seeks to identify unique star formation sites from
present-day stellar abundances.
Previous techniques have treated each abundance dimension as being
statistically independent, despite theoretical expectations that
many elements can be produced by more than one nucleosynthetic process.
In this work we introduce a data-driven model of nucleosynthesis 
where a set of latent factors (e.g., nucleosynthetic yields) contribute
to all stars with different scores, and clustering (e.g., chemical tagging) 
is represented by a mixture model of multivariate gaussians
in a lower-dimensional latent space.
We use an exact method to simultaneously estimate the factor scores for
each star, the partial assignment of each star
to each cluster, and the latent factors common to all stars.
We use an information-theoretic Bayesian principle to estimate the number of
latent factors and clusters.
We perform experiments on generated and real data, including literature chemical abundances
of metal-poor stars, and those from the second \Galah\ data release. 
We find that four latent factors are preferred to explain the detailed 
abundances of metal-poor stars, and five latent factors are preferred given 
16 chemical abundances in \Galah\ data. In both experiments we recover latent factors that 
are representative of mean yields expected from the rapid- and slow-neutron
capture process, as well as nucleosynthesis in Type II supernovae. 
However, in both experiments we also identify latent factors that link
the production of light (e.g., K, Al) and heavy (e.g., Ba, Eu) elements.
We find three clusters are preferred given 1,136 stars in \Galah,
two of which are representative of the [$\alpha$/Fe]-poor and 
[$\alpha$/Fe]-rich populations in the Milky Way disk. 
%The third is consistent
%\todo{with what.}. 
We release accompanying software that scales well with the available data,
allowing for model parameters to be optimised in seconds given a fixed number
of latent factors, components, and $\sim$$10^{7.5}$ abundance measurements.
%We release software that scales \todo{as $\mathcal{O}(XX)$}, where $X$ is \todo{what},
%enabling model parameter estimation in minutes given $\sim$$10^7$
%chemical abundance measurements.
\end{abstract}


\keywords{methods: statistical}

\section{Introduction} \label{sec:intro}

The detailed chemical abundances that are observable in a star's photosphere provide a
fossil record that carries with it information about where and when that star
formed \citep{Freeman;Bland-Hawthorn:2002}. While the photospheric abundances remain largely unchanged throughout
a star's lifetime \citep[however see][]{Dotter:2017,Ness:2018b}, the dynamical 
dissipation timescale of open clusters in the Milky Way disc is of order a few 
gigayears \citep{Portegies-Zwart:1998}. That makes chemical tagging an attractive 
approach to identify star formation sites long after those stars are no longer 
gravitationally bound to each other.


Gravitationally bound star clusters have been useful laboratories for
testing the limits and utility of chemical tagging. Although biases arise when
only considering star clusters that are still gravitationally bound, the chemical
homogeneity of open clusters provides an empirical measure of how similar stars
would need to be before they could be tagged as belonging to the same
star formation site \citep{Mitschang:2014}. However, there are analysis
issues in understanding how precisely those chemical abundances can be measured
\citep{Bovy:2016}, and how chemically similar stars can be which did not form 
together \citep[doppleg\"angers;][]{Ness:2018}.
If open clusters were truely chemically homogeneous then under idealistic 
assumptions our ability to chemically tag the Milky Way would depend primarily
on the precision with which we can measure those chemical abundances in stars. 
Data-driven approaches to modelling stellar spectra are
improving upon this precision \citep{Ness:2015,Ness:2018a,Ness:2018b,
Casey:2016,Casey:2017,Ho:2017b,Ho:2017a,Leung;Bovy:2018}, but more work is
needed: astronomers have not yet developed unbiased estimators of chemical
abundances that saturate the Cram\'er-Rao bound.


%There are further chemical tagging obstacles to overcome once all the chemical 
%abundance information in a stellar spectrum can be extracted. Most notably is a 
Chemical tagging experiments require a catalogue of precise chemical abundance
measurements for a large number of stars,
where those chemical abundances trace different nucleosynthetic pathways.
This is the primary goal of the \Galah\ survey \citep{DeSilva:2015,Martell:2017,Buder:2018},
a stellar spectroscopic survey that uses the High Efficiency and Resolution 
Multi-Element Spectrograph \citep[\project{HERMES};][]{Sheinis:2015} on the Australian 
Astronomical Telescope (AAT).  \Galah\ will observe up to $10^6$ stars in the 
Milky Way, and measure up to 30 chemical abundances for each star. This includes
light odd-Z elements (e.g., Na, K), elements produced through
alpha-particle capture (e.g., Mg, Ca, Ti), and elements produced
through the slow (e.g., Ba) and rapid neutron-capture process
(e.g., Eu). No other spectroscopic survey provides an equivalent set of
chemical abundances for a comparable number of stars.

%These data provide an unparalleled view on the production
%of chemical elements in the Milky Way.


Given these data and the most favourable assumptions in chemical tagging 
-- that star clusters are truely chemically homogenous, and we can measure 
those abundances with infinite precision, and those abundances are 
differentiable between star clusters -- then chemical
tagging becomes a clustering problem. All clustering techniques applied to 
chemical tagging thus far have assumed that the data dimensions are independent. That is to say
that adding a dimension of [Ni/H] provides entirely new, independent information
that could not have been predicted from other elemental abundances.
Theory and observations agree that this cannot be true.
Nucleosynthetic processes produce multiple elements in varying
quantities, and the effective dimensionality of stellar abundance datasets has been shown
to be lower than the actual number of dimensions \citep{Ting:2012,Price-Jones:2018}.
Any clustering approach that treats each new elemental abundance as an 
independent axis of information will therefore conclude with biased inferences
about the star formation history of our Galaxy. 


%Alternative approaches are perhaps more challenging, as i
It is not trivial to estimate
the nucleosynthetic yields that have contributed to the chemical abundances of each star
with great confidence. There are
qualitative statements that can be made for large numbers of stars, or particular
types of stars, but quantifying the precise contribution of different processes
to each star is an unsolved problem. For example, the so-called [$\alpha$/Fe] `knee' in
abundance ratios in the Milky Way can qualitatively be explained by 
core-collapse supernovae being the predominant nucleosynthetic process in the
early Milky Way before Type Ia supernovae had a significant contribution, but 
efforts to date have not sought to try to explain the detailed abundances of a 
single star as a contribution of yields from different systems (e.g., 20\% of the
Ba was produced by 3 M$_\odot$ asymptotic giant branch stars). This is in part 
because of the challenging and degenerate nature of the problem as described, 
and is complicated by the differences in yield predictions that account from 
prescriptions used in different theoretical models.


New approaches to chemical tagging are clearly needed. Immediate advances would
include methods that take the dependence among chemical elements into account
within some generative model, or techniques that combine chemical abundances
with dynamical constraints to place joint prior probabilities on whether any
two stars could have formed from the same star cluster, given some model of the
Milky Way. 

In this work we focus on the former.
Here we present a new approach to chemical tagging that allows us to identify 
the latent (unobserved) factors that contribute to the chemical abundances of 
all stars (e.g., nucleosynthetic yields) while simultaneously performing 
clustering in the latent space. Notwithstanding caveats that we will
discuss in detail, this allows us to infer nucleosynthetic yields rather than
strictly prescribe them from models. Moreover, the scale of the clustering
problem reduces by a significant fraction because the clustering is performed in
a lower dimensional latent space instead of the higher dimensional data space.
In Section~\ref{sec:methods} we describe the model and the methods we use to
estimate the model parameters. Section~\ref{sec:experiments} describe the 
experiments performed using generated and real data sets. We discuss the results
of these experiments in Section~\ref{sec:discussion}, including the caveats with
the model as described. We conclude in Section~\ref{sec:conclusions}.



\section{Methods} \label{sec:methods}

Factor analysis is a common statistical approach for describing correlated 
observations with a lower number of latent variables \citep[e.g.,][]{Thompson:2004}.
Related techniques include principal component analysis \citep{Hotelling:1933} and its
variants \citep{Tipping;Bishop:1999}, singular value decomposition \citep{Golub:1970}, and other
matrix factorization methods. While factor analysis on its own is a useful
dimensionality reduction tool to identify latent factors that contribute to
the chemical abundances of stars \citep[e.g.,][]{Ting:2012,Price-Jones:2018}, factor
analysis cannot describe clustering in the data (or latent) space.
Similarly, clustering techniques applied to chemical abundances to date 
\citep[e.g.,][]{Hogg:2016} do not account for the lower dimensionality of the
data. 

Here we expand on a variant of factor analysis known elsewhere as a mixture of common 
factor analyzers \citep{Baek:2010}, where the data are generated by a set of 
latent factors that are common to all data, but the scoring (or extent) of those
factors is different for each data point, and the data can be modelled as a
mixture of multivariate normal distributions in the latent space (factor scores).
In this work the data $\vecdataunscaled$ is a 
$\NumData \times \NumDimensions$ matrix where $\NumData$ is the number of 
stars and $\NumDimensions$ is the number of chemical abundances measured 
for each star. We assume a generative model for the data 
\begin{equation}
	\vecdataunscaled = \vec\mu + \factorloads\factorscores + \vec{e}
	\label{eq:generative-model}
\end{equation}

\noindent{}where $\factorloads$ is a $\NumLatentFactors \times \NumDimensions$ 
matrix of factor loads that is common to all data points, $\NumLatentFactors$ is
the number of latent factors, and the factor scores 
for the $\numdata$th data point
\begin{equation}
	\factorscores_\numdata \sim \mathcal{N}(\vec\xi_\numcomponents, \vec\Omega_\numcomponents)
\end{equation}
\noindent{}are drawn from the $\numcomponents$th multivariate normal distribution.
The mean vector $\vec\mu$ describes the mean datum in each dimension.
The factor scores for all data points $\factorscores$ is then a 
$\NumData \times \NumLatentFactors$ matrix, where each data point has a partial
association to the components in latent space. 
We assume $\vec{e} \sim \mathcal{N}\left(\vec{0}, \textrm{diag}(\specificvariance)\right)$
is independent of the latent space, and $\specificvariance$ is a
vector of variances in each $\NumDimensions$ abundance dimensions.
In this model each data point can be represented as being drawn
from a mixture of multivariate normal components, except the components
are \emph{clustered in the latent space} $\factorscores$ and projected
into the data space by the factor loads $\factorloads$. 


We assume that the latent space is lower dimensionality than the
data space (e.g., $\NumLatentFactors < \NumDimensions$).
Within the context of stellar abundances, the factor loads
$\factorloads$ can be thought of as the \emph{mean} yields
of nucleosynthetic
events (e.g., $s$-process production from AGB stars averaged over
initial mass function and star formation history), and the
factor scores are analogous to the relative counts of those 
nucleosynthetic events. The clustering in factor scores
achieves the same as a clustering procedure in data space,
except we simultaneously estimate the latent processes that are
common to all stars (the so-called factor loads, analogous to 
nucleosynthetic yields). Within this framework a rare nucleosynthetic event
can still be described as a `factor load' $\factorloads_\numlatentfactors$, 
but its rarity would be represented by associated factor
scores being zero for most stars and thus have negligible contribution
to the observed abundances. In practice the factor loads can only be 
identified up to orthogonality and cannot be expressly interpreted as
nucleosynthetic yields because they have limited physical meaning
(we discuss this further in Section~\ref{sec:discussion}),
but this description of typical yields and relative event rates should
help build intuition for the model parameters, and provide context
within the astrophysical problem it is being applied.


Including latent factors in the model description allows us to account for 
processes that affect multiple elemental abundances. In this way we are 
accounting for the fact that the data dimensions are not independent of
each other. Another benefit is the scaling with computational cost. If we 
considered data sets of order $10^{7.5}$
entries (e.g., 30 chemical abundances for $10^6$ stars) purely as a
clustering problem, then even the most efficient clustering
algorithms would incur a significant cumulative computational 
overhead by searching the parameter space for the number of
clusters, and the optimal model parameters given that number
of components. However, because the mixture of factor analyzers
approach assumes that there is a \emph{lower dimensional latent 
space} in which the data are clustered, and that clustering is 
projected into real space by common factor loads, the 
dimensionality of the clustering problem is reduced from 
$N \times D$ to $N \times J$. This reduces computational cost through
faster execution of each optimization step, and on average fewer optimization steps
needed to reach a specified convergence threshold.

From a statistical standpoint, the primary advantage to using
a mixture of factor analysers is that we can simultaneously
estimate latent factors (e.g., infer nucleosynthetic 
yields) and perform clustering (e.g., chemical tagging) 
within a statistically consistent framework. That is to say
that we have a generative data-driven model that can 
quantitatively describe nucleosynthetic yields, and the
factor scores describe the variance in turbulence and gas mixing,
or star formation efficiency, and the parameters of this model
can be simultaneously estimated in a self-consistent way.

Without loss of generality the density of the mean-subtracted 
data $\vecdataunscaled - \vec\mu$ (which we hereafter will refer to simply as $\vecdata$) can be described as
\begin{equation}
	f(\vecdata; \vec\Psi) = \sum_{\numcomponents=1}^{\NumComponents}\weight_\numcomponents\phi(\vecdata;\factorloads\scoremeans_\numcomponents, \factorloads\scorecovs_\numcomponents\factorloads\transpose + \diag{\specificvariance})
	%\eye\specificvariance
\end{equation}
\noindent{}given $\NumLatentFactors$ common factor loadings and $\NumComponents$ components
clustered in the latent (factor score) space. Here the parameter
vector
$\vec\Psi$ includes $\{\factorloads,\vec\pi,\scoremeans,\scorecovs,\specificvariance\}$, and $\phi(\vecdata; \vec\theta)$
describes the density of a multivariate gaussian distribution,
and $\weight_\numcomponents$ describes the relative weighting of the $\numcomponents$th
component in latent space and $\sum\vec\weight = 1$.
The log likelihood is then given by
\begin{equation}
	\log\mathcal{L}(\vecdata|\vec\Psi) = \sum_{\numcomponents=1}^{\NumComponents}\log{f(\vecdata;\vec\Psi)} \quad . \label{eq:log-likelihood}
\end{equation}


The model as described is indeterminate in that there is no unique 
solution for the factor loads $\factorloads$ and scores
$\factorscores$. These quantities can only be determined up until 
orthogonality in $\factorloads$. However, as we will describe in Section \ref{sec:expectation-maximization}, with suitable priors on $\vec\Psi$ 
one can efficiently estimate the model parameters using the expectation-maximization
algorithm \citep{Dempster:1977}. 
% TODO: say something about "..with a suitable number of initial guesses?"


\subsection{Initialisation} \label{sec:initialisation}

Here we describe the default initialisation of the model parameters, which
we have found to be robust in a large set of experiments. Given a number of
$\NumLatentFactors$ latent factors we initialise the factor load
$\factorloads$ entries to be drawn from a uniform distribution
$\factorloads \sim \mathcal{U}\left(-1, +1\right)$ and then ensure that
the factor loads are orthogonal such that
%TODO: Should we re-write this so that we just draw from the scipy special ortho group?
\begin{equation}
	\factorloads \factorloads\transpose = \eye \quad .
\end{equation}
We then initially assign each data point as belonging to one of the
$\NumComponents$ components using the \texttt{k-means++} algorithm \citep{Arthur:2007}.
During the Experiments described in Section~\ref{sec:experiments} we repeat
this initialisation procedure 25 times for every permutation of $\NumLatentFactors$ and $\NumComponents$. We run expectation-maximization (Section~\ref{sec:expectation-maximization})
from each initialisation until convergence and adopt the model with the
highest log likelihood as the preferred model for that permutation of $\NumLatentFactors$ and $\NumComponents$.

Given the initial
factor loads and assignments, we then estimate the relative weights
$\vec\pi$, the mean factor scores of each component $\scoremeans$, and
the covariance matrix of factor scores of each component $\scorecovs$.
Finally, we initialise the specific variance $\specificvariance$ in each
dimension as the variance in each data dimension. Other initialisation 
methods are available in the code associated with this \emph{Article}. 
%Throughout this work we found that the optimised model parameters appear
%to be insensitive to the choice of initialisation up until orthogonal
%rotation of the latent space.


\subsection{Expectation-Maximization} \label{sec:expectation-maximization}

We use the expectation-maximization algorithm to estimate the model parameters
\citep{Dempster:1977}. With each expectation step we evaluate the log likelihood 
given the model parameters $\vec\Psi$ and we calculate the $\NumData \times \NumComponents$ responsibility 
matrix $\vec\tau$ whose entries are the posterior probability that the 
$\numdata$th data point is associated to the $\numcomponents$th component, given 
the data $\vecdata$ and the current estimate of the parameter vector $\vec\Psi$:
\begin{equation}
	\tau_{\numdata\numcomponents} = \frac{\weight_\numcomponents\phi(\vecdata_\numdata;\factorloads\scoremeans_\numcomponents, \factorloads\scorecovs_\numcomponents\factorloads\transpose + \diag{\specificvariance})}{\sum_{g=1}^{G}\weight_g\phi(\vecdata_\numdata;\factorloads\scoremeans_g, \factorloads\scorecovs_g\factorloads\transpose + \diag{\specificvariance})} \quad .
\end{equation}


At the maximization step we update our estimates of the parameters $\vec\Psi$,
conditioned on the data $\vecdata$ and the responsibility matrix $\vec\tau$.
The updated parameters estimates are found by setting the second derivative
of the log likelihood (Eq.~\ref{eq:log-likelihood}) to zero and solving for
the parameter values\footnote{Strictly this introduces a statistical inconsistency in that we should update our parameter estimates by setting the second derivative of our chosen objective function (Eq.~\ref{eq:message-length}) to zero instead of the log likelihood, but this inconsistency only becomes serious with small $N$.}
In doing so this guarantees that every updated
estimate of the model parameters is guaranteed to increase the log likelihood.
Although there are no guarantees against converging on local minima, in 
practice it is sufficient to run expectation-maximization from multiple
initialisations (as we do) in order to ensure that the global minima is reached.
At the maximization step we first update our estimate of the relative weights 
$\vec\weight\nextstep$ given the responsibility matrix $\vec\tau$
\begin{equation}
	\weight_\numcomponents\nextstep = \frac{1}{\NumData} \sum_{\numdata=1}^{\NumData}\tau_{\numdata\numcomponents}
	% Include proof.
\end{equation}
\noindent{}where the $\vec{\Psi}\thisstep$ superscript refers to the current parameter estimates and $\vec{\Psi}\nextstep$ refers to the updated estimate for the next iteration.
The updated estimates of the mean factor scores 
$\scoremeans\nextstep$ for each component are then given by
\begin{eqnarray}
	\scoremeans_\numcomponents\nextstep = \scoremeans_\numcomponents\thisstep + \frac{\vec{G}\transpose(\vecdata\transpose - \factorloads\thisstep\scoremeans_\numcomponents\thisstep)\vec\tau_\numcomponents}{\NumData\weight_\numcomponents\nextstep}
\end{eqnarray}
\noindent{}where:
\begin{eqnarray}
	\vec{W} &=& (\scorecovs_\numcomponents\thisstep)^{-1}\eye \\
	\vec{V} &=& \left(\specificvariance\thisstep\right)^{-1} \\
	\vec{C} &=& (\vec{W} + (\factorloads\thisstep)\transpose\vec{V}\factorloads\thisstep)^{-1}\eye \\
	\vec{G} &=& \left[\vec{V} - \vec{V}\factorloads\thisstep\vec{C}\left(\vec{V}\factorloads\thisstep\right)\transpose\right]\factorloads\thisstep\scorecovs_k\thisstep \quad .
\end{eqnarray}

The covariance matrices of the components of factor scores $\scorecovs\nextstep$
are updated next,
\begin{equation}
	\scorecovs_\numcomponents\nextstep = \left(\eye - \vec{G}\transpose\factorloads\thisstep\right)\scorecovs_\numcomponents\thisstep + \frac{\vec{G}\transpose\vec{Z}\left(\vec{Z}\vec\tau_\numcomponents\transpose\right)\transpose\vec{G}}{N\weight_\numcomponents\nextstep}
\end{equation}
\noindent{}where
\begin{eqnarray}
	\vec{Z} &=& \vecdata\transpose - \factorloads\thisstep\scoremeans_\numcomponents\thisstep \quad .
\end{eqnarray}

After some linear algebra, updated estimates of the common factor loads $\factorloads\nextstep$
can be found from
\begin{equation}
	\factorloads\nextstep = \factorloads_{a}\left(\factorloads_{b}^{-1}\eye\right)
\end{equation}
\noindent{}where:
\begin{eqnarray}
	\factorloads_\textrm{a} &=& \sum_{\numcomponents=1}^{\NumComponents}\left[ \vec\tau_\numcomponents\transpose\vecdata\left(\scoremeans_\numcomponents\thisstep\right)\transpose + 
	\vec{G}\transpose\vec\tau_\numcomponents\vec{Z}\transpose\vec{G}\right] \\
	\factorloads_\textrm{b} &=& N\sum_{\numcomponents=1}^{\NumComponents}\left[\weight_\numcomponents\nextstep\left(\scorecovs_\numcomponents\nextstep + \scoremeans_\numcomponents\nextstep\left(\scoremeans_\numcomponents\nextstep\right)\transpose\right)\right] \quad
\end{eqnarray}


Finally, the updated estimate of the specific variances $\specificvariance\nextstep$ is given
by
\begin{equation}
	\specificvariance\nextstep = \frac{1}{\NumData}\left[\sum^{\NumComponents}_{\numcomponents=1}\vec\tau_\numcomponents\transpose\left(\vecdata\odot\vecdata\right) - \sum_{j=1}^{J}\left(\factorloads\nextstep\factorloads_\textrm{b}\right)\odot\factorloads\nextstep\right]
\end{equation}

\noindent{}where $\odot$ denotes is the entry-wise (Hadamard) product. We repeat
the expectation-maximization cycle for up to 10,000 steps or until the log likelihood
improved by less than $10^{-5}$ between successive iterations. 
Our software implementation allows for a positive regularisation term to be added
along the diagonal of covariance matrices in latent space $\scorecovs$, for all experiments
here we set this term to zero. Similarly, we assume that the data are noiseless and
we do not add any observed errors to the constructed covariance matrices.
%Although our implementation
%allows for a positive regularisation term to be added along the diagonal of the covariance
%matrices in latent space $\scorecovs$, we set this regularisation term to zero for all
%experiments performed here. Similarly, we assume that the data are noiseless and we do
%not add any observed errors to the constructed covariance matrices.




\subsection{Model Selection}

The expectation-maximization algorithm as described requires a specified 
number of latent factors $\NumLatentFactors$ and $\NumComponents$. In the next 
Section we describe experiments using generated and real data, and in all cases we will
assume that the true number of latent factors and components are not known. 
Hence we require some criterion to decide how many latent factors, and how many
components, are preferred given some data. An increasing number of factors
and components will undoubtedly increase the log likelihood of the model given
the data, but the log likelihood does not account for the increased model 
complexity that is afforded by those additional latent factors and components.


One criterion commonly employed for evaluating a class of models is the 
Bayesian Information Criterion \citep[BIC;][]{Schwarz:1978}, 
\begin{equation}
	\textrm{BIC} = Q\log{N} - 2\log\mathcal{L}\left(\data|\vec\Psi\right) \quad , \label{eq:bic}
\end{equation} 
\noindent{}where $Q$ is the number of model parameters:
\begin{equation}
	Q = \frac{\NumLatentFactors}{2}\left[2\left(\NumDimensions - \NumLatentFactors\right) + \NumComponents\left(3 + \NumLatentFactors\right)\right] + \NumComponents + \NumDimensions - 1 \quad .
\end{equation}
\noindent{}We evaluate the BIC for all experiments performed in this work.
However, while the BIC does include a penalisation term for the number of
parameters (which scales with $\log{N}$), it does not describe for the
increased flexibility that is afforded by the addition of those parameters.
For example, adding one parameter
to a model will increase the BIC by at most $\log{N}$, but there are different
ways for a single parameter to be introduced. In a fictitious model $y=f(x)$
a parameter $b$ could be added that is a scalar multiple of $x$, or it could be
introduced as $x^b$. Despite the difference in model complexity, the same
penalisation occurs in BIC. Even if the log likelihood were only to improve
marginally in both cases, the difference in model complexity is not captured
by BIC. In other words, there are situations where we are more interested in
balancing the model complexity (or the expected Fisher information and similar
properties) with the goodness of fit, instead of penalising the number of 
parameters.


For these reasons we also calculate the Minimum Message Length \citep[MML;][]{Wallace:2005}
for every model as a criterion for model selection and evaluation. 
The classically-described princple of MML is that the best explanation of the
data given a model is the one that leads to the shortest so-called two-part message~\citep{Wallace:2005}, 
where a \textit{message} takes into account both the complexity of the model 
and its explanatory power. The complexity of the model is described through
the first part of the message, and the second part of the message describes
its explanatory power. The \emph{length} of each message part is quantified
(or estimated) using information theory, allowing for a fair evaluation between
different models of varying complexity or explanatory power. MML has been 
shown to perform well on a variety of empirical analyses (see, e.g., 
\cite{viswanathan1999finding,fitzgibbon2004minimum}, with references to 
further examples in \cite{Wallace:2005,dowe2007bayes,Dowe2008a,Dowe2011a}).
Arguments about the statistical consistency (i.e.,~as the number of data 
points increases the distributions of the estimates become increasingly 
concentrated near the true value) of MML are given in \cite{DoweWallace1997a,Dowe2011a}.
The MML principle requires that we explicitly specify our prior beliefs on the
model parameters, providing a Bayesian optimisation approach which can be
applied across entire classes of models.

The \textit{message} must encode two parts: the model, and the data given the
model. The encoding of the message is based on Shannon's information theory~\citep{Shannon:1948}. 
The information gained from an event $e$ occurring, where $p(e)$ is the
probability of that event, is $I(e) = -\log_{2}{p(e)}$. The information content
is largest for improbable outcomes, and smallest for outcomes that we are 
almost certain about. In other words, an outcome that has a probability close
to unity has nearly zero information content because almost nothing new is learned from it,
whereas rarer events convey a much higher information content. 





In practice calculating the message length can be a non-trivial task, 
especially for models that are reasonably complex. This makes the strict MML
principle intractable (or uncomputable) in many cases and necessitates
approximations to the message length. Using a Taylor expansion, a generalised
scheme can be calculated to estimate the parameter vector $\vec\Psi$ that
minimises the message length ${I}(\vec\Psi,\vecdata)$ \citep{Wallace:1987},
\begin{equation}
	{I}(\vec\Psi,\vecdata) = \frac{Q}{2}\log\kappa_Q - \log\left(\frac{p(\vec\Psi)}{\sqrt{|\mathcal{F}(\vec\Psi)|}}\right) - \log\mathcal{L}\left(\vecdata|\vec\Psi\right) + \frac{Q}{2} \label{eq:mml} 
\end{equation}
\noindent{}where $\log\likelihood(\vecdata|\vec\Psi)$ is the familiar
log likelihood, $p(\vec\Psi)$ is the joint prior density on $\vec\Psi$,
$\mathcal{F}(\vec\Psi)$ is the negative second derivative of the log likelihood,
commonly referred to as the expected Fisher information matrix,
\begin{equation}
	\mathcal{F}(\vec\Psi) = -\textrm{E}\left[\frac{\partial^2}{\partial\vec\Psi^2}\log\likelihood(\vecdata|\vec\Psi)\right]
\end{equation}
\noindent{}and as before $Q$ is the number of model parameters.
Continuous parameters can only be stated to finite precision, which leads
to the $\frac{Q}{2}\log\kappa_Q$ term that gives a measure of the volume of the region of
uncertainty in which the parameters $\vec\Psi$ are centred. The $\log\kappa_Q$
term is
\begin{equation}
	\log\kappa_Q = -\log{2\pi} + \frac{1}{Q}\log{Q\pi} - \gamma - 1
\end{equation}
\noindent{}where $\gamma$ is Euler's constant.

Like the BIC, the message length is penalised by the number of model parameters
through the $\log\kappa_Q$ term. However, the model complexity is also 
described through the priors and the Fisher information matrix, which
describes the curvature of the log likelihood with respect to the model
parameters. For these reasons, MML provides a more accurate description of
the model complexity (or flexibility) because it naturally includes the 
curvature of the log likelihood with respect to the model parameters
rather than only penalising models based on the \emph{number} of
parameters.

%\todo{Dowe said something about MML reducing to BIC under some conditions, but I have not yet found the right paper to cite.}

\begin{figure*}
	\includegraphics[width=1.0\textwidth]{experiments/exp1-data-colour.pdf}
    \caption{The generated data for Experiment~1, where points are coloured by their
		     true associations to each component.}
    \label{fig:exp1-data}
\end{figure*}



We will describe the contributions to the message length in parts. We assume
the priors on the number of latent factors $\NumLatentFactors$ and the
number of components $\NumComponents$ to be
$p(\NumLatentFactors) \propto 2^{-\NumLatentFactors}$ and
$p(\NumComponents) \propto 2^{-\NumComponents}$ respectively,
such that fewer numbers are preferred and the optimal lossless message to 
encode the number of each is \citep[p279, sec. 6.8.2;][]{Knorr-Held:2000},
\begin{eqnarray}
	I(\NumLatentFactors) &= -\log{p(\NumLatentFactors)} &= \NumLatentFactors\log{2} + \textrm{constant} \label{eq:prior_J} \\
	I(\NumComponents) &= -\log{p(\NumComponents)} &= \NumComponents\log{2} + \textrm{constant} \label{eq:prior_K} \quad .
\end{eqnarray}
Only $\NumComponents - 1$ of the relative weights $\vec\weight$ need encoding because 
$\sum_{\numcomponents=1}^{\NumComponents} = 1$. We assume a uniform prior on 
individual weights,
\begin{equation}
	p(\vec\weight) = (\NumComponents - 1)! \quad ,
\end{equation}
\noindent{}and the Fisher information is
\begin{equation}
	\mathcal{F}(\vec\weight) = \frac{\NumData^{\NumComponents - 1}}{\prod_{\numcomponents=1}^{\NumComponents}\weight_\numcomponents} \quad ,
\end{equation}
\noindent{}which gives the message length of the relative weights $I(\vec\weight)$ to be
\begin{eqnarray}
	I(\vec\weight) &=& -\log\left(\frac{p(\vec\weight)}{\sqrt{|\mathcal{F}(\vec\weight)|}}\right) \nonumber \\
			   &=& -\log{p(\vec\weight)} - \frac{1}{2}\log{|\mathcal{F}(\vec\weight)|} \nonumber  \\
			   &=& -\log(\NumComponents - 1)! + \frac{\NumComponents - 1}{2}\log{\NumData} - \frac{1}{2}\sum_{\numcomponents=1}^{\NumComponents}\log\weight_\numcomponents \nonumber \\
	I(\vec\weight) &=& \frac{1}{2}\left(\left(\NumComponents - 1\right)\log\NumData - \sum_{\numcomponents=1}^{\NumComponents}\log\weight_\numcomponents\right) -\log\Gamma(\NumComponents) \quad . \nonumber \label{eq:prior_pi} \\
\end{eqnarray}

We assume uniform priors for the component means in latent space $\scoremeans$, 
and a conjugate inverted Wishart prior for the component covariance matrices
$\scorecovs$ \citep[Section 5.2.3;][]{Knorr-Held:2000},
\begin{equation}
	p(\scoremeans_\numcomponents,\scorecovs_\numcomponents) \propto |\scorecovs_\numcomponents|^{\frac{1}{2}(\NumLatentFactors + 1)} \quad .
\end{equation}
We approximate the determinate of the Fisher information of a multivariate normal $|\mathcal{F}(\scoremeans,\scorecovs)|$
as $|\mathcal{F}(\scoremeans)||\mathcal{F}(\scorecovs)|$ \citep{Oliver:1996,Figueiredo:2002} where
\begin{equation}
	|\mathcal{F}(\scoremeans)| = (\NumData\weight_k)^\NumLatentFactors|\scorecovs_k|^{-1}
\end{equation}
\begin{equation}
	|\mathcal{F}(\scorecovs)| = (\NumData\weight_k)^{\frac{1}{2}\NumLatentFactors(\NumLatentFactors+1)}2^{-\NumLatentFactors}|\scorecovs_k|^{-(\NumData+1)}
\end{equation}
\noindent{}such that 
% todo put line in equation.
\begin{eqnarray}
	I(\scoremeans,\scorecovs) &=& -\sum_{\numcomponents=1}^{\NumComponents}\log{p(\scoremeans_k,\scorecovs_k)} + \frac{1}{2}\sum_{\numcomponents=1}^{\NumComponents}\log{|\mathcal{F}(\scoremeans_k,\scorecovs_k)|} \nonumber \\
	&=& \frac{1}{2}\sum_{\numcomponents=1}^{\NumComponents}\log\left[(\NumData\weight_k)^{\frac{1}{2}\NumLatentFactors(\NumLatentFactors+3)}2^{-\NumData}|\scorecovs_k|^{-(\NumData + 2)}\right] \nonumber \\
	&& \cdots \,\, -\sum_{\numcomponents=1}^{\NumComponents}\log{|\scorecovs_k|}^{\frac{1}{2}(\NumLatentFactors + 1)} \\
I(\scoremeans,\scorecovs) &=& \frac{1}{4}\NumLatentFactors(\NumLatentFactors+3)\sum_{\numcomponents=1}^\NumComponents\log{\NumData\weight_k} - \frac{KD}{2}\log{2} \nonumber \\ 
&& \cdots \,\, -\frac{1}{2}(2\NumLatentFactors+3)\sum_{k=1}^{K}\log{|\scorecovs_k|}  \quad . \label{eq:prior_xi_omega} 
\end{eqnarray}

Previous work on multiple latent factor analysis within the context of MML have
addressed the indeterminacy between the factor loads and factor scores by
placing a joint prior on the product of factor loads and scores, whilst
ensuring that the factor loads themselves remain mutually orthogonal \citep{WallaceMLF}.
Adopting the same prior density in our model is not practical because 
it would require the priors $p(\scoremeans|\vec\tau,\vec\weight$) and $p(\scorecovs|\vec\tau,\vec\weight$).
That is, we would require a prior density on both the means $\scoremeans$
and covariance matrices $\scorecovs$ in latent space that requires knowledge
about the responsibility matrix $\vec\tau$ and relative weights $\vec\weight$ in order to estimate the effective scores
$\factorscores$ for each data point and calculate a joint prior on the product
of the factor loads $\factorloads$ and factor scores $\factorscores$.
Instead we address this indeterminacy
by placing a prior on $\factorloads$ that ensures it is mutually orthogonal.
Specifically, we adopt a Wishart distribution with scale matrix $\vec{W}$
and $D$ degrees of freedom for the
$\NumLatentFactors\times\NumLatentFactors$ matrix $\vec{M} = \factorloads\transpose\factorloads$.
In other words, $\vec{M} \sim W_\NumLatentFactors(D,\vec{W})$
and $\vec{W} = \textrm{Cov}(\factorloads\transpose)$.
This
Wishart joint prior density gives highest support for mutually orthogonal vectors,
\begin{equation}
	p(\factorloads) = \frac{|\factorloads\transpose\factorloads|^{\frac{1}{2}(\NumDimensions - \NumLatentFactors - 1)}}{2^{\frac{\NumDimensions\NumLatentFactors}{2}}|\vec{W}|^{\frac{\NumDimensions}{2}}\Gamma(\frac{\NumDimensions}{2})}\exp\left[-\frac{1}{2}\textrm{Tr}(\vec{W}^{-1}\factorloads\transpose\factorloads)\right] \quad .
\end{equation}

%\todo{Fisher:}
%\begin{widetext}
%\begin{eqnarray}
%	\log\likelihood(\vecdata|\vec\Psi_k) = -\frac{1}{2}(\vecdata - \factorloads\vec\xi_k)\transpose(\factorloads\omega_k\factorloads\transpose + \diag{D})^{-1}) - \frac{1}{2}\log|(\factorloads\omega_k\factorloads\transpose + \diag{D})| -\frac{1}{2}\log{2\pi}
%	\log\likelihood(\vecdata|\vec\Psi) &=& -\frac{1}{2}(\vecdata - \vec{M})\transpose\vec\Sigma^{-1}(\vecdata - \vec{M}) - \frac{1}{2}\log|\vec\Sigma| - \frac{1}{2}\log{2\pi} \quad \mathrm{where} \\
%	\vec{M} &=& \factorloads\scoremeans \\
%	\vec\Sigma &=& \factorloads\scorecovs\factorloads\transpose + \diag{\specificvariance}
%\end{eqnarray}
%\end{widetext}

The Fisher information for $\factorloads$ \todo{needs to be checked.}
Thus the message length to encode $\factorloads$ is given by
\begin{eqnarray}
	I(\factorloads) &=& -\log\left(\frac{p(\factorloads)}{\sqrt{|\mathcal{F}(\factorloads)|}}\right) \nonumber \\
I(\factorloads)	&=& \frac{1}{2}\textrm{Tr}(\textrm{Cov}(\factorloads\transpose)^{-1}\factorloads\transpose\factorloads) - \frac{1}{2}(\NumDimensions - \NumLatentFactors - 1)\log{|\factorloads\transpose\factorloads|} \nonumber \\
	&& \cdots \,\, + \frac{1}{2}\NumDimensions\NumLatentFactors\log{2} + \frac{1}{2}\NumDimensions\log{|\textrm{Cov(\factorloads)}|} - \Gamma\left(\frac{\NumDimensions}{2}\right) \nonumber \\
	&& \cdots \todo{+\frac{1}{2}\log{|\mathcal{F}(\factorloads)|}} \quad . \label{eq:prior_L}
\end{eqnarray}

Combining equations \ref{eq:prior_J}, \ref{eq:prior_K}, \ref{eq:prior_pi}, \ref{eq:prior_xi_omega}, and \ref{eq:prior_L} with equation \ref{eq:mml} leads to the full message length:
\begin{widetext}
\begin{eqnarray}
	I(\vec\Psi, \vec\data) &=& -\log\likelihood(\vec\data|\vec\Psi)
 +\frac{1}{4}\left(\NumLatentFactors + 4\right)\left(\NumLatentFactors - 1\right)\sum_{\numcomponents=1}^\NumComponents\log\weight_\numcomponents + \left(\NumComponents - \frac{1}{2}\right)\log{\NumData}
 +\frac{1}{2}\NumDimensions\log|\textrm{Cov}\left(\factorloads\transpose\right)| \nonumber \\
  && \cdots -\frac{1}{2}\left(D-J-1\right)\log|\factorloads\transpose\factorloads| + \textrm{Tr}\left(\textrm{Cov}\left(\factorloads\transpose\right)^{-1}\factorloads\transpose\factorloads\right) - \left(\NumLatentFactors + \frac{3}{2}\right)\sum_{\numcomponents=1}^\NumComponents\log|\scorecovs_\numcomponents|  -\log\Gamma\left(\NumComponents\right) - \Gamma\left(\frac{\NumDimensions}{2}\right) \nonumber \\
&& \cdots +\frac{Q}{2}\log\kappa_q +\frac{1}{2}\left[\NumLatentFactors(\NumDimensions + 2) + \NumComponents(2-\NumData)\right]\log{2} + \todo{\frac{1}{2}\log{|\mathcal{F}(\factorloads)|}} \quad . \label{eq:message-length}
 \end{eqnarray}
\end{widetext}


% -\log\likelihood(\vec\data|\vec\Psi) \
% + (J + K + \frac{KN}{2} + \frac{DJ}{2})\log{2}     --> \frac{1}{2}(3(J+K) + ND)\log{2}
% 0.5*((K-1) logN - \sum\log\weight)  + \frac{1}{4}J(J+3)\sum\log(N\weight) --> 
% -\log\gamma(K) - \Gamma(D/2)
% -\frac{1}{2}(2J+3)\sum\log|\omega_k|
% + factor loads stuff
% + Q/2\log\kappa(q)
 





%\clearpage

\section{Experiments} \label{sec:experiments}




\subsection{Experiment 1: Toy model with generated data} \label{sec:exp-1}




We generated a data set with ${\NumData = 1}$00,000 data points, each with
$\NumDimensions = 15$ dimensions. We adopted a latent dimensional space of 
$\NumLatentFactors = 5$ ($\NumLatentFactors \times \NumDimensions$) factor loads, 
with $\NumComponents = 20$ clusters in the latent space. The relative weights $\vec\weight$
are drawn from a multinomial distribution and the means of the clusters
in factor scores $\scoremeans$ are drawn from a standard normal
distribution. The off-diagonal entries in the covariance matrices in factor scores $\scorecovs$ are drawn from a gamma distribution $\scorecovs_{\numcomponents,i,i} \sim \vec\Gamma\left(1\right)$. The variance in 
each dimension $\specificvariance$ are also drawn $\specificvariance \sim \vec\Gamma\left(1\right)$.
We randomly draw a  $\NumDimensions \times \NumDimensions$ matrix from a Haar distribution \citep{Haar:1933},
which is uniform on the special orthogonal group $\textrm{SO}(n)$ and therefore guaranteed to return an orthogonal
matrix with a determinant of unity \citep{Stewart:1980}.
We denote the $\NumLatentFactors \times \NumDimensions$ left-most region of this
matrix to be $\mathbf{H}$, and by taking $\factorloads_\ast$ to be the Cholesky decomposition of $\mathbf{H}\transpose \mathbf{H}$, we find the factor loads
\begin{eqnarray}
 	\factorloads = \mathbf{H}\left(\left(\factorloads_\ast\right)^{-1}\eye\right)
\end{eqnarray}
\noindent{}which ensures that the true factor loads is a $\NumLatentFactors \times \NumDimensions$
matrix of mutually orthogonal vectors and that $\factorloads\transpose \factorloads = \eye$.
The $\numdata$th data point (which belongs to the $\numcomponents$th cluster) is then
generated by drawing $\factorscores_{\numdata} \sim \mathcal{N}(\scoremeans_\numcomponents,\scorecovs_\numcomponents)$, projecting by the factor loads $\factorloads$, and adding variance $\specificvariance$.




\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp1-gridsearch-ll-contours.pdf}
	\includegraphics[width=0.45\textwidth]{experiments/exp1-gridsearch-bic-contours.pdf}
	\includegraphics[width=0.45\textwidth]{experiments/exp1-gridsearch-mml-contours.pdf}
    \caption{The top panel shows the negative log likelihood 
			 $-\log{\mathcal{L}\left(\data|\vec\Psi\right)}$ 
			 evaluated at each combination of latent factors 
			 $\NumLatentFactors$ and number of clusters 
			 $\NumComponents$ using the generated data in
			 Experiment~1.  The middle panel shows 
			 the BIC (Eq.~\ref{eq:bic}) for those 
			 combinations, and the lower panel shows the 
			 message length. The white marker indicates the
			 lowest value in each panel, showing the
			 preferred number of latent factors and components.
			 The black marker indicates the true value.}
    \label{fig:experiment-1-gridsearch}
\end{figure}



\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp1-latent.pdf}
    \caption{The inferred latent scores $\factorscores$ for Experiment~1
    		 from a model with $\NumLatentFactors = 5$ latent factors and
		 	 $\NumComponents = 20$ multivariate normal components in
			 latent space.
             Points are coloured by the inferred component.} 
    \label{fig:exp1-latent}
\end{figure*}


Here we treat the generated data set as if the true number of latent factors
and the true number of components are not known. Starting with $\NumLatentFactors = 1$
and $\NumComponents = 1$, we trialled each permutation of $\NumLatentFactors$ and $\NumComponents$
until $\NumLatentFactors_\textrm{max} = 10$
and   $\NumComponents_\textrm{max} = 40$ (e.g., twice $\NumLatentFactors_\textrm{true}$ and $\NumComponents_\textrm{true}$).
%For each $\NumLatentFactors$, $\NumComponents$ permutation we initialised the factor
%loads and component assignments randomly. 
We initialised the model as per Section~\ref{sec:initialisation} and performed expectation-maximization cycles until the relative log likelihood improved by less than $10^{-5}$. 


We recorded the \emph{negative} log likelihood, the BIC, and the message length for each permutation of $\NumLatentFactors$ and $\NumComponents$.
These metrics are shown in Figure~\ref{fig:experiment-1-gridsearch}.
Unsurprisingly the negative log likelihood increases with increasing numbers of latent
factors $\NumLatentFactors$ and increasing numbers of components $\NumComponents$.
The lowest BIC value and message length is found at $\NumLatentFactors = 5$
and $\NumComponents = 20$, identical to the true values.

\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp1-compare-all.pdf}
    \caption{The estimated factor loads $\factorloads$ (left), factor scores $\factorscores$ (middle),
    		 and specific variances $\specificvariance$ (right) compared to the 
		 	 true data generating values
		 	 for Experiment~1. The agreement is excellent.}
    \label{fig:exp1-compare}
\end{figure*}


We show a corner plot of the generated data in Figure~\ref{fig:exp1-data},
demonstrating the clustering (colour) and structure varying structure in data space.
Despite this clustering in data space, it is clear from Figure~\ref{fig:experiment-1-gridsearch} that a combination of latent factors
and allowing clustering in the latent space provides a better description of the (generated) data.
Adding components to the model improves the log likelihood, even with a single latent factor,
but the addition of just \emph{one latent factor} improvers the log likelihood more than adding
\emph{twenty components}. Not much more can be said for this example because the true data generating process 
is drawn from a higher number of latent factors, but this toy model does illustrate how 
clustering in high dimensional data can be better described by latent factors with 
clustering in the lower dimensional latent space.
The inferred clustering in latent space is shown in Figure~\ref{fig:exp1-latent},
where each point is coloured by the inferred associated component. 




Some technical discussion is warranted before we compare our estimated model
parameters to the true values. The latent factors in this model are only
identifiable up to an orthogonal rotation. That is to say that
if the data were truely generated by latent factors $\factorloads_\textrm{true}$,
then our estimates of those latent factors $\factorloads_\textrm{est}$ do not need
to be identical to the true values. For example, the ordering of the estimated factors
could be different from the true factors, and the ordering of the dimensionality
in latent space would be correspondingly different. Since no constraint is
placed on the ordering of the factor loads during expectation-maximization,
there is no assurance (or requirement) that our factor loads match the true factor loads.






Another possibility is that the estimated factor loads could be flipped in sign 
relative to the true factor loads, and the scores would similarly be flipped. 
In both of these situations (reordering or flipped signs) the log likelihood 
given the data and the estimated factor loads $\factorloads_\textrm{est}$ 
%\citep[or Kullback-Leibler divergence;][]{Kullback:1951}
would be identical to the log likelihood given the data and the true factor loads 
$\factorloads_\textrm{true}$
despite the difference in ordering and sign. The same can be said for any other
scalar metric \citep[e.g., Kullback-Leibler divergence;][]{Kullback:1951}.
These examples serve to illustrate a more 
general property that the factor loads and factor scores can be orthogonally 
rotated by \emph{any valid rotation matrix}\footnote{A rotation matrix is valid if 
$\vec{R}\vec{R}\transpose = \vec{I}$} $\vec{R}$. The estimated factor loads 
$\factorloads_\textrm{est}$ could therefore appear very different from the true 
values, but they only differ by an orthogonal rotation. We discuss the impact of this limitation on real data in more detail in Section~\ref{sec:discussion}. 




We took the model with the preferred number of latent factors and components found
from a grid search ($\NumComponents = 20$, $\NumLatentFactors = 5$; which are also
the true values) and applied an orthogonal rotation to the latent space to be as
close as possible to the true values. The rotation matrix $\mathbf{R}$ was found
by solving for $\NumLatentFactors$ unknown angle parameters, each of which is used
to construct a Givens rotation matrix \citep{Givens:1958}, and then we take the product of those Givens
matrices. This process reduces to Euler angle rotation in three or fewer dimensions.
This process rotates the latent space
($\factorloads$, $\scoremeans$, $\scorecovs$), but has no effect on the model's 
predictive power: the generated data, evaluated log likelihood, or the Kullback-Leibler divergence \citep{Kullback:1951} under the
rotated model is indistinguishable from the unrotated model.
In Figure~\ref{fig:exp1-compare} we show the estimated factor loads $\factorloads$,
factor scores $\factorscores$, and specific variances $\specificvariance$ compared
to the true values. The agreement is excellent in all model parameters.

\vspace{2em}

 


\subsection{Experiment~2:\\Inferring astrophysically realistic latent factors}

In this experiment we use detailed chemical abundances of metal-poor stars 
from \citet{Barklem:2005} to illustrate the utility and limitations
in interpreting the latent factors as nucleosynthetic processes. The data 
can be described as 14 chemical abundance measurements from 61 stars ($\NumData = 61$, $\NumDimensions = 14$), and those 14 chemical elements trace multiple nucleosynthetic pathways. Specifically the chemical
abundances measured include Al (light odd-Z), Mg and Ca ($\alpha$-process), Sc, Ti, Cr, Mn, Fe, Co, and Ni (Fe-peak elements), as well as Sr, Y, Ba, and Eu ($s$/$r$-process). 
The data vector $\vecdata$ includes [Fe/H] abundance ratios and all other elements
relative to Fe (e.g., [Ti/Fe]) after subtracting the mean value in each dimension.

We performed a grid search for the optimal number of latent factors 
$\NumLatentFactors$ and components $\NumComponents$, starting from 
$\NumLatentFactors = 1$ and $K = 1$, and trialling up until 
$\NumLatentFactors = 7$ and $\NumComponents = 3$. 
% Although it is common
%to add a regularization term to the diagonals of covariance matrices
%when performing expectation-maximization, we found that our existing
%grid search boundaries were sufficient to find the preferred $\NumLatentFactors$
%and $\NumComponents$, and chose not to add any regularization term.
%We initialised
%the model parameters randomly for each permutation of $J$ and $K$, and continued
%the expectation-maximization algorithm
%until a convergence threshold of $10^{-5}$ was reached in the log likelihood. 
In Figure~\ref{fig:exp2-gridsearch-contours} we show the negative log likelihood,
the BIC, and the message length recorded for the optimised set of model parameters 
for each permutation of $J$ and $K$. The negative log likelihood continues to
increase with increasing numbers of clusters, and increasing numbers of latent
factors.
The lowest BIC value is found for $J = 4$ and $K = 2$, indicating
four latent factors are preferred with two clusters. The shortest message length
is also found with four latent factors, but with no clustering ($J = 4$, $K = 1$).
We adopt the message length as our objective function and chose $J = 4$ and $K = 1$
as the preferred model.


\begin{comment}
With this model we sought to find a rotation matrix that would allow us to 
identify plausible astrophysical processes in the factor loads.
Although having to perform this rotation is comfortable, there is utility in
trying to identify the latent factors because the factors can only be rotated by
some valid rotation matrix. That is to say that we will not only exactly what
we have put in: the rotated factors will simply be the closest set of factor
loads that are consistent with the data, allowing us to identify the factor
loads that are astrophysically plausible. We constructed
a $J \times D$ astrophysical factor load matrix $\factorloads_\textrm{A}$ where
all entries were zero except for the following entries:
%\begin{itemize}
%	\item the entry corresponding to Al in the first latent factor to represent
%		  odd-Z element production,
%	\item the entries corresponding to Ca and Mg in the second latent factor to
%		  represent $\alpha$ particle production,
%	\item the entries corresponding to Ni, Co, Fe, Mn, Cr, Ti, and Sc in the third
%		  latent factor to represent the Fe-peak group,
%	\item the entries corresponding to Sr, Y, and Ba in the fourth latent factor
%		  to represent products of the slow neutron capture process, and
%	\item the entries corresponding to Eu in the fifth latent factor to represent
%		  products of the rapid nucleosynthesis process.
%\end{itemize}
\todo{\begin{itemize}
	\item the entries corresponding to C, Al, Ca, Mg, Ni, Co, Fe, Mn, Cr, Ti, and Sc in the first
		  latent factor to represent the products of core-collapse supernovae,
	\item the entries corresponding to Sr, Y, Ba, and Eu in the second latent factor
		  to represent products of the neutron capture process.
\end{itemize}
}
These entries were set to $1/\sqrt{E}$ where $E$ is the number of non-zero entries
in that latent factor, in order to ensure that $\factorloads_\textrm{A}$ is
mutually orthogonal. We then found the valid rotation matrix $\vec{R}$ that most
closely approximated the astrophysical loads $\factorloads_\textrm{A}$.
In Figure~\ref{fig:exp2-factor-loads} we show the rotated factor loads found by our
preferred model, and illustrate the relative contributions (positive or negative)
to each element.
\end{comment}

\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp2-\ExperimentHash-gridsearch-ll.pdf}
	\includegraphics[width=0.45\textwidth]{experiments/exp2-\ExperimentHash-gridsearch-bic.pdf}
	\includegraphics[width=0.45\textwidth]{experiments/exp2-\ExperimentHash-gridsearch-mml.pdf}
    \caption{The top panel shows the negative log likelihood 
			 $-\log{\mathcal{L}\left(\data|\vec\Psi\right)}$ 
			 evaluated at each combination of latent factors 
			 $\NumLatentFactors$ and number of clusters 
			 $\NumComponents$ using the \citet{Barklem:2005}
			 data in Experiment~2.  The middle panel shows 
			 the BIC (Eq.~\ref{eq:bic}) for those 
			 combinations, and the lower panel shows the 
			 message length. The white marker indicates the
			 lowest value in each panel, showing the
			 preferred number of latent factors and components.}
    \label{fig:exp2-gridsearch-contours}
\end{figure}

\begin{comment}
The rotated loads are not precisely the same as the target astrophysical loads
in part because the target loads only seek to \emph{identify} and \emph{order}
factors that can be interpreted within an astrophysical context. 
We discuss this in more detail in Section~\ref{sec:discussion}.
\end{comment}

%\todo{For example,
%the first factor does show positive contributions to Al, like the target load,
%but it also shows a slight negative contribution with respect to alpha elements
%(Mg, Ca)}, and a gradual increase among Fe-peak elements up to Ni. Similarly while
%Ca and Mg are indeed positive in the second factor, as targeted, there is a
%weak negative contribution to Mn abundances, which was not part of the target
%loads. 
We applied an orthogonal rotation to the estimated latent factors such that we
could make qualitative comparisons between experiments (e.g., so we can discuss
the closest estimated latent factors between different experiments).
Despite the caveats in interpreting latent factors as nucleosynthetic processes
(e.g., see Sections~\ref{sec:exp-1} and \ref{sec:discussion}), here we will
qualitatively describe the latent factors found in our selected model (Figure~\ref{fig:exp2-factor-loads}).
The absolute\footnote{We discuss non-zero entries because a negative factor loads could have only negative factor scores, and their multiplication would produce a positive abundance.}
entries of first factor load $\mathbf{L}_0$ has strong non-zero entries at Eu, Ba, Sr, and Al.
The strong link with Eu is suggestive that this latent factor is consistent with being
the rapid neutron-capture process.
However, some of these factor load entries may be non-zero because we require the latent factors to
be mutually orthogonal, and not because they truly contribute to the data. To try and
disentangle these possibilities, we calculate the fractional contribution that factor
load makes to the observed abundances relative to other factor loads. We define the
fractional contribution of the $j$th factor load to the $d$th
data dimension as:
\begin{equation}
% $|\mathbf{C}_{d}|$
	\mathbf{C}_\textrm{d,j} = \frac{\sum^{N}|\factorloads_{j,d}\factorscores_{n,j}|}{\sum^J\sum^{N}|\factorloads_{j,d}\factorscores_{n,j}|} \quad .
\end{equation}
The fractional contributions to each element are shown in the right hand side of
Figure~\ref{fig:exp2-factor-loads}. Here it is clear that despite that the first factor load $|\mathbf{L}_0|$ has support at Ba and Sr, it is not the dominant factor that contributes to those elements.
However, we also find that Al abundances are primarily explained by this factor. 

The second factor load $\mathbf{L}_1$ closely represents the slow neutron capture process.
Here the contributions to Ba, Y, and Sr are all highest for this factor load. We
also find the Fe-peak element Ni to have the highest fractional contribution from this factor,
but Ni has a near uniform mix of contributions from different factors.
The third and fourth factors $\mathbf{L}_2$ show non-zero support for a mix of elements
that are produced by $\alpha$-particle capture reactions. Mg, Ca, and Ti all have their
highest contributions from $\mathbf{L}_3$, as do Sc and Co, whereas the iron-peak elements
Cr, Mn, and Fe all have their largest contributions from $\mathbf{L}_2$.


\begin{comment}
The first latent factor $|\mathbf{L_0}|$ shows a non-zero contribution to most
elements (Figure~\ref{fig:exp2-factor-loads}). The elements Ca, Sc, Ti, Cr, and Co, mostly
Fe-peak elements, have the largest contribution from this factor. We interpret this latent
factor as being most similar to expected contributions from core-collapse supernovae.
The second latent factor $|\mathbf{L_1}|$ has negligible contribution at most elements,
with strong contributions for C, and the neutron-capture elements Sr, Y, Ba, and Eu. We
interpret this latent factor as being most closely representative of the slow neutron
capture process, which primarily occurs in asymptotic giant branch stars. The third latent factor has almost no contribution to Ba, but has
non-zero contributions to other neutron-capture elements Sr, Y, and Eu. Since Eu has
the strongest contribution by $|\mathbf{L_2}|$, we interpret this factor as being most
representative of the rapid neutron capture process. The last latent factor $|\mathbf{L_3}|$
has a noticeable peak at Fe and the surrounding Fe-peak elements, with small non-zero contributions at most elements. \todo{What to say of $|\mathbf{L_3}|$?}

The first factor shows a positive contributions to most elements except for
neutron-capture products, where the contribution decreases with increasing
atomic number. The second latent factor shows strong contributions to neutron
capture process elements -- with this latent factor being responsible for almost
all Eu -- and shows a predominant contribution to Sc. Scandium is one of the most
discrepant elements between observations and models of galactic chemical evolution,
as models under-predict [Sc/Fe] ratios by a factor of ten \citep[e.g.,][]{Kobayashi:2006,Casey:2015}.
\end{comment}

% Sr, Y, Ba, and to a
%lesser extent, Eu, which is representative of slow neutron capture production.
%Similarly, the last component shows a very strong positive contribution towards
%Eu, and could be interpreted as production from the rapid nucleosynthesis process.

\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp2-\ExperimentHash-latent-factors-and-contributions.pdf}
	\caption{Latent factors found in Experiment~2 using the \citet{Barklem:2005}
			 data. The panels on the left show the absolute entries for each factor load. 
			 On the right we show the absolute fractional contributions to
			 each element, illustrating the loads that contribute most to each element.}
    \label{fig:exp2-factor-loads}
\end{figure*}



After accounting for the contributions by the latent factors and scores,
the specific scatter remaining in each dimension is shown in
Figure~\ref{fig:exp2-specific-scatter}. The remaining scatter varies from as low 
as 0.05~dex (Mg, Ca, Ti, Y) up to 0.25~dex (Ba). 
 A large relative scatter might suggest that there are other latent
factors that contribute to those elements, but those latent factors were not included because
the message length preferred a fewer number of factors, or it could be limitations in
the assumptions of our model (e.g., we assume mutually orthogonal latent factors).
%However, the scatter in 
%Fe and Ni is somewhat surprising. Those elements usually have strong and numerous atomic
%lines that can be easily measured in metal-poor stars. 


%The relatively wide uncertainty regions on these factors is perhaps not
%surprising given that only 61 stars were used in this data set. However, this
%experiment hopefully demonstrates that the rotated latent factors have some
%interpretability. 

\begin{figure}
	\includegraphics[width=0.5\textwidth]{../experiments/exp2-\ExperimentHash-specific-scatter.pdf}
	\caption{Specific scatter $\sqrt{\specificvariance}$ remaining in the \citet{Barklem:2005} data used in
			 Experiment~2 after accounting for the contributions by all latent
			 factors.}
    \label{fig:exp2-specific-scatter}
\end{figure}



\subsection{Experiment 3:\\Chemical abundances in the \Galah\ survey}
\label{sec:exp4}

In this experiment we perform blind chemical tagging using the 
photospheric abundances released as part of the second \Galah\ 
data release \citep{Buder:2018}. This data set includes
up to 23 chemical abundances reported for 342,682 stars.
Some abundances are not reported for a variety of 
reasons related to astrophysics or the adopted analyses
(e.g., a star is too hot to estimate the abundance for a given
electronic transition, or the signal-to-noise of the spectrum is too low).
Our method as described cannot efficiently handle missing data 
entries for a given star, and as such we are forced to exclude stars that
do not have a full complement of abundances. However, we also
chose to exclude some elemental abundances from our chemical
tagging experiments. For example, our experiments
with \Galah\ data do not include lithium or carbon abundances because the
photospheric varies throughout a star's lifetime (e.g., see Experiment 2). This is true
to a small degree for many elements \citep[e.g.,][]{Dotter:2017},
but for the purposes of this experiment we assume that all other
photospheric abundances remain constant throughout a star's
lifetime.


\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp3-\ExperimentHash-gridsearch-ll.pdf}
	\includegraphics[width=0.45\textwidth]{experiments/exp3-\ExperimentHash-gridsearch-bic.pdf}
	\includegraphics[width=0.45\textwidth]{experiments/exp3-\ExperimentHash-gridsearch-mml.pdf}
    \caption{The top panel shows the negative log likelihood 
			 $-\log{\mathcal{L}\left(\data|\vec\Psi\right)}$ 
			 evaluated at each combination of latent factors 
			 $\NumLatentFactors$ and number of clusters 
			 $\NumComponents$ using \Galah\ data in
			 Experiment~3.  The middle panel shows 
			 the BIC for those combinations, and the lower panel shows the 
			 message length. The white marker indicates the
			 lowest value in each panel, showing the
			 preferred number of latent factors and components.}
    \label{fig:exp3-gridsearch}
\end{figure}

\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp3-\ExperimentHash-latent-factors-and-contributions.pdf}
	\caption{Latent factors inferred in Experiment~3 using the \Galah\
			 data. The panels on the left show the absolute entries for each
			 factor load. On the right we show the absolute fractional contributions
			 to each element, illustrating the loads that contribute most to each
			 element.}
    \label{fig:exp3-factor-loads}
\end{figure*}



We first selected stars with \texttt{flag\_cannon = 0} to exclude
stars where there is reason to suspect that the stellar parameters
(e.g., $\teff$, $\logg$) are unreliable, and as a result the 
detailed chemical abundances would be untrustworthy. We then took all
stars with no erroneous flags in the following elemental abundance
measurements: Na, Al, Si, K, Ca, Sc, Ti, V, Mn, Fe, Ni, Cu, Zn, Ba, La, and Eu.
These elements are produced from multiple nucleosynthetic
pathways, and it is this subsample that we will use for 
our initial chemical experiment with \Galah\ data.
There were 1,136 stars that met these criteria. 

%["Mg", "Al", "Ca", "Sc", "Ti", "Cr", "Mn", "Fe", "Co", "Ni", "Y", "Ba", "Eu"]
\begin{comment}
We ordered the
list of chemical abundances by the number of reliable measurements,
which is shown in Figure~\ref{fig:exp4-abundance-counts}. The number of
independent abundance measurements (e.g., where there is no reason
to suspect a single abundance to be untrustworthy) is shown in black,
and the cumulative number of trustworthy abundances is shown in blue.
The cumulative line can be interpreted as the number of stars for
which the full set of elemental abundances (up until and including
the abundance on the x-axis) is reported. We chose to include the
18 most commonly reported abundances (up to and including Al),
leaving us with a sample of \todo{50,000} stars.
\end{comment}


\begin{comment}
\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp3-abundance-counts.pdf}
	\caption{The number of abundance measurements in the second
		     \Galah\ data release \citep{Buder:2018} where \texttt{flag\_cannon = 0}
		     and \texttt{flag\_<element> = 0}. The black line indicates the number
		     of reported measurements for a single element, and the blue line
		     indicates the number of stars where all elements leftward (inclusive)
		     are reported. The dashed vertical line indicates the top 18 reported
		     elemental abundances, which we use for Experiment~4.}
    \label{fig:exp3-abundance-counts} 
\end{figure}
\end{comment}


We executed a grid search for the number of latent factors $\NumLatentFactors$
and the number of components $\NumComponents$ that were preferred by the data.
Starting with $\NumLatentFactors = 1$ and $\NumComponents = 1$, we trialled each
permutation of $\NumLatentFactors$ and $\NumComponents$ up until $\NumLatentFactors = 7$
and $\NumComponents = 5$. 
The results of this grid search are shown in Figure~\ref{fig:exp3-gridsearch},
where we show the negative log likelihood, the BIC, and message length found for each permutation.
The model with $J = 5$ and $K = 2$ is found to have the shortest message length.


\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp3-\ExperimentHash-specific-scatter.pdf}
	\caption{Specific scatter $\sqrt{\specificvariance}$ remaining in the \Galah\ data \citep{Buder:2018}
			 in Experiment~3 after accounting for the contributions by all
			 latent factors.}
    \label{fig:exp3-specific-scatter}
\end{figure}




\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp3-\ExperimentHash-latent-space.pdf}
	\caption{The factor scores $\factorscores$ estimated in Experiment~3 using the \Galah\ data \citep{Buder:2018}, where each star is coloured by its inferred component.}
    \label{fig:exp3-latent-space}
\end{figure*}


\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp3-\ExperimentHash-data-space.pdf}
	\caption{Detailed chemical abundances from \Galah\ \citep{Buder:2018} that were used in Experiment~3. Each star is coloured by its inferred component in latent space.}
    \label{fig:exp3-data-space}
\end{figure*}


The latent factors inferred from this model are shown in Figure~\ref{fig:exp3-factor-loads},
where we observe some similarities to the factors identified using the \citet{Barklem:2005}
data of metal-poor stars. The first factor $\mathbf{L}_0$ is the dominant contributor to
Eu and La, both heavy elements, but is also the dominant contributor for the Fe-peak
element Sc. Unlike experiment 2 where $\mathbf{L}_0$ is the dominant contributor to Al, here $\mathbf{L}_0$ has a negligible contribution to Al.

The second latent factor $\mathbf{L}_1$ here is most representative of the slow neutron capture process, with dominant contributions to Ba and Zn. This latent factor has some support at V, but with
relatively little contribution. However, interestingly we find that the production of K (a light odd-Z element) is almost entirely dominated by this latent factor. Indeed, the fractional contribution this
factor makes to K is higher than the fractional contributions it makes to Ba or Zn.

The Fe-peak elements V, Mn, Fe, Ni, and Cu are all dominated by the third latent factor $\mathbf{L}_2$.
Although the entries $\mathbf{L}_2$ are lower for light odd-Z elements Na and Al, we surprisingly find that this
factor also makes the dominant contributions to Na and Al. For the typically used tracers of $\alpha$-element production -- Si, Ca, and Ti -- we find that all three elements have strong dominant contributions by $\mathbf{L}_3$. The last factor, $\mathbf{L}_4$ has non-zero support that is highest at the slow neutron
capture element tracers Zn, Na, and La, but this latent factor is not a dominant contributor to any single element. 


%The specific scatter after accounting for these latent factors is smallest
%for Fe and largest for K and Y (Figure~\ref{fig:exp9-specific-scatter}).

In Figure~\ref{fig:exp3-latent-space} we show the inferred clustering in latent space,
where the separation between components is arguably best seen in the splitting between $\factorscores_1$ with $\factorscores_2$ and $\factorscores_3$. When projected to data space (Figure~\ref{fig:exp3-data-space}) the third component (purple) is seen to have relatively higher abundance ratios of [K,Ba,Zn/Fe] at a given [Fe/H],
and lower abundance ratios of [V/Fe]. This is consistent with the clustering in latent space, where $\factorloads_1$ is the dominant contributor to K, Ba, and Zn, and $\factorloads_2$ is the dominant contributor to the iron-peak elements (including V).

With a larger sample of stars that have a complete abundance inventory we might expect to 
identify more components. However
because our model requires no missing abundances in any element, we would
need to reduce the number of elemental abundances we include in order to increase
the sample of stars. And by doing so we restrict the number of latent factors
that can be inferred from the data.
The specific scatter after accounting for all latent factors is smallest for Fe (0.01~dex), with
K and Ba showing the largest scatter (0.13~dex; Figure~\ref{fig:exp3-specific-scatter}). 



\section{Discussion} \label{sec:discussion}

Literature studies testing the limitations chemical tagging have shown
that the effective dimensionality of chemical abundance space is lower than
the number of reported abundances \citep[e.g.,][]{Ting:2012, Price-Jones:2018}. 
This is expected. Theoretical models of stellar evolution and nucleosynthesis
would suggest that there are of order at most ten important nucleosynthetic 
contributors that are responsible for the production of the $\sim$20--30
chemical abundances typically reported. As a consequence we can expect that 
the observed chemical abundances are correlated in some way (e.g., by some
linear combination of a fewer number of nucleosynthetic pathways).  However, 
most approaches to chemical tagging to date have assumed that the observed 
chemical abundances are independent of each other. This problem cannot be
resolved by simply lowering the number of abundances used for chemical tagging (e.g.,
down to something near the effective dimensionality), either, because those
chemical abundances are still produced by multiple pathways.

Here we have introduced a model to simultaneously account for the lower
effective dimensionality of chemical abundance space, and perform clustering
in that lower dimensional space. This provides a data-driven model of
nucleosynthesis yields and chemical tagging that allows us to simultaneously
estimate the latent factors that contribute to all stars, and cluster those 
stars by their relative contributions from each factors. The results are
encouraging in that we find latent factors that are representative of the
expected yields from dominant nucleosynthetic channels. However, the model that
we describe is very likely \emph{not} the correct model to use to represent 
chemical abundances of stars. Here we discuss the limitations of our model 
in detail.


We require latent factors to be mutually orthogonal to in order to resolve
the indeterminacy in our model. This suggests an astrophysical context where 
the mean nucleosynthetic yields (integrated over all stellar masses and star
formation histories) of various nucleosynthetic processes (e.g., $r$-process, 
$s$-process) are mutually orthogonal to each other. Clearly this assumption 
is likely to be incorrect: the nuclear physics
of one environment where elements are produced will be very different from
others, and there is no astrophysical constraint that those
yields (or latent factors) should be mutually orthogonal.
One could represent them by a hierarchical data-driven model where the yields contribute as a function of stellar mass, 
metallicity, and other factors), but in principle to resolve the indeterminacy
the mean of these yields would require mutual orthogonality. Introducing a constraint on the factor scores that resolves this indeterminacy and allows for more flexible latent factors would be a worthy extension to this work.


The constraint of mutual orthogonality limits the inferences we want to make
about stellar nucleosynthetic yields. For example, after accounting for all
known sources of potassium production in the Milky Way, galactic chemical evolution
models under-predict the level of potassium in the Milky Way by more than an
order of magnitude \citep{Kobayashi:2006}. From our inferences in Experiment~3
using \Galah\ data, we find that $\factorloads_1$ is the dominant contributor to
potassium, but this factor load is also the dominant contributor to Ba and Zn,
elements that primarily trace the slow neutron capture process. Does this
suggest that the production of potassium is linked to the s-process?
If our model could confidently and
reliably associate the production of potassium with the slow neutron capture
process then it could help explain the peculiar abundances of stars enhanced in
potassium and depleted in magnesium \citep{Mucciarelli:2012,Cohen:2012} -- a chemical
abundance pattern that currently lacks explanation \citep{Kemp:2018}.
However, is the potassium contribution that we infer physically realistic,
or is it a consequence of requiring that the latent factors are mutually
orthogonal? Distinguishing these possibilities is non-trivial, which is in
part why caution is warranted when trying to interpret latent factor models.
In this situation it is worth commenting that potassium has the largest specific
scatter (Figure~\ref{fig:exp3-specific-scatter}), suggesting that the contributions
of potassium are not as well described by the latent factor model as other
elements.


The same arguments hold for the contributions to aluminium by $\factorloads_0$ in 
Experiment~2. This factor is the dominant contributor to Eu, an element that
is almost entirely produced by the r-process, yet is also the dominant contributor
to Al, a light odd-Z element. The specific scatter for Al in Experiment 2 is 
among the highest observed (nearly 0.2~dex), which would suggest that the link
between Al and Eu is not physically realistic and that Al is not well-described
by the combination of latent factors in Experiment 2. Indeed, when we make use
of a larger set of precise Al measurements in Experiment 3, we find that Al has
almost no contribution by the 'r-process factor load` $\mathbf{L}_0$, and its
dominant contributor is $\mathbf{L}_2$, the same factor load that dominates
the contributions to the other odd-Z element Na.


There are other issues in our model that relate to our assumption of mutual
orthogonality. Even if nucleosynthetic yields were truely mutually orthogonal,
then the latent factors we infer are only \emph{identifiable} up until an
orthogonal basis. As we have seen in our experiments, the ordering and sign 
of the latent factors is not described \emph{a priori}. That means that we
must `assign' the latent factors we infer as being described by an astrophysical
process (e.g., the zeroth-latent factor is r-process). A more general limitation
of this is that the latent factors can be multiplied by some arbitrary rotation
matrix, leading to latent factor loads that are very different from what was
estimated by the model, but still lead to the exact same data (or log likelihood,
or Kullback-Liebler divergence, etc). As a consequence, we can only `identify'
latent factors up until this rotation. We have sought to address this by constructing
rotation matrices where the entries each latent factor correspond to our expectations
from astrophysical processes (whilst remaining orthogonal), but here we are limited
by what astrophysical processes we are \emph{expected} to find within the constraint
of being mutually orthogonal.


This in part constrains our ability to identify new nucleosynthetic processes. For example,
let us consider a hypothetical situation where we would only expect there to be four 
nucleosynthetic processes that predominately contribute to the observed \Galah\ abundances,
but in practice we found that the data are best explained with
five latent factors. We construct a rotation matrix where the first four latent factors
describe the mean nucleosynthetic processes we expect to find. What of the fifth latent
factor? We can constrain the possible values of the fifth latent factor conditioned on
the requirement that all factors remain mutually orthogonal, but one can imagine that
some (or perhaps many) elements have entries where the fifth latent factor can have
near-zero or zero entries. Even if the mean nucleosynthetic yields are mutually
orthogonal, there are scenarios that one can imagine where there is a limited amount
we can say with confidence about that new nucleosynthetic process.

Notwithstanding these issues, we have presented experiments that demonstrate
that a latent factor model which allows for clustering in latent space can adequately
describe chemical abundance data. Our experiments with toy data (Experiment~1) reliably
finds the true number of latent factors and cluster components when we adopt an information-theoretic approach to the objective function (e.g., BIC or MML), and correctly estimates the latent
model parameters. When applied to chemical abundances measured from metal-poor stars 
(Experiment~2) we find that four latent factors are preferred by the data, and
those latent factors qualitatively re-appear in Experiment~3 using \Galah\ data. 
That did not have to be the case: the latent factors between different experiments (data sets)
might be entirely different, and those factors did not have to qualitatively match our
expectations of nucleosynthetic yields. Indeed, the inferred factors -- even after a
valid rotation -- could have made no astrophysical sense whatsoever. For this reason it
is encouraging that there is some interpretability in the latent factors, even though 
caution is warranted.

The relatively small data set with complete abundances measured in \Galah\ both limits
the number of components we can infer, and the efficacy of chemical tagging. Only three
components were preferred from the data set of 1,136 stars, which we identified as
being stars with low- and high-[$\alpha$/Fe] abundance ratios, and a third component that
primarily differs in its abundances of K, Ba, Zn, and V. A worthwhile extension
of this work would be to incorporate missing and noisy data in order to allow for
blind chemical tagging on an industrial scale, allowing for the latent factors to
be efficiently estimated while chemical tagging occurs in the lower-dimensional
latent space.

\section{Conclusions} \label{sec:conclusions}

We have introduced a data-driven model of nucleosynthesis by incorporating 
latent factors that are common to all stars, and allowing for clustering in the
lower-dimensional latent space. This approach simultaneously allows us to efficiently
tag stars based on their chemical abundances, and to infer the contributions that are
common to all stars (e.g., nucleosynthetic yields). Experiments with generated data
demonstrate that MML is a useful principle for identifying the preferred number of
latent factors and components. Experiments with real data reveal latent factors
that are qualitatively similar to expected nucleosynthetic yields (e.g., products
from the $s$-process, $r$-process, et cetera). Similar latent factors emerge 
from chemical abundances of metal-poor stars and using data from the second \Galah\
data release. While we express caution in physically interpreting those latent factors, our model does provide the first data-driven approach to nucleosynthesis and chemical tagging.

% This paper was built from reference XXX on github repo YYY


\acknowledgements
We acknowledge support from the Australian Research Council
through Discovery Project DP160100637.
The \Galah\ survey is based on observations made at the Australian Astronomical Observatory, under programmes A/2013B/13,
A/2014A/25, A/2015A/19, A/2017A/18. We acknowledge the traditional owners of the land on which the AAT stands, the Gamilaraay
people, and pay our respects to elders past and present.
%This work presents results from the European Space Agency (ESA) space mission Gaia. Gaia data is being processed by the Gaia Data Processing and Analysis Consortium (DPAC). Funding for the DPAC is provided by national institutions, in particular the institutions participating in the Gaia MultiLateral Agreement (MLA). The Gaia mission website is https://www.cosmos.esa.int/gaia . The Gaia archive website is https://archives.esac.esa.int/gaia .
This research has made use of NASA's Astrophysics Data System.


\software{
	\package{Astropy}\,\,\citep{astropy:v1,astropy:v2},\,\,
    \package{IPython}\,\,\citep{ipython},\,\,
    \package{matplotlib}\,\,\citep{mpl},\,\,
    \package{numpy}\,\,\citep{numpy},\,\,
    \package{scipy}\,\,\citep{scipy},\,\,
    \package{Stan}\,\,\citep{stan},\,\,
    \package{Jupyter Notebooks}\,\,\citep{jupyter-notebooks}\,\,
}    % TOPCAT



\bibliographystyle{aasjournal}
\bibliography{mcfa}

\end{document}
