\documentclass[twocolumn]{aastex61}
\usepackage{bm}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{color}
\usepackage{comment}

\newcommand\teff{T_{\rm eff}}
\newcommand\logg{\log{g}}
\newcommand\feh{[\rm{Fe}/\rm{H}]}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\package}[1]{\texttt{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\article}{\emph{Article}}

\newcommand{\Gaia}{\project{Gaia}}
\newcommand{\gaia}{\project{gaia}}
\newcommand{\Galah}{\project{Galah}}
\newcommand{\GALAH}{\project{GALAH}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}


\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\renewcommand{\vec}[1]{\vect{#1}}

\newcommand{\weight}{\pi}
\newcommand{\data}{\textbf{Y}}
\newcommand{\vecdata}{\vec\data}

\newcommand{\nextstep}{^\textrm{(t+1)}}
\newcommand{\thisstep}{^\textrm{(t)}}
\newcommand{\transpose}{^\intercal}
\newcommand{\eye}{\textbf{I}}

\newcommand{\factorloads}{\textbf{L}}
\newcommand{\factorscores}{\textbf{S}}
\newcommand{\specificvariance}{\vec{D}}

\newcommand{\scoremeans}{\vec\xi}
\newcommand{\scorecovs}{\vec\Omega}

\newcommand{\NumData}{N}
\newcommand{\NumDimensions}{D}
\newcommand{\numdata}{n}
\newcommand{\numdimensions}{d}
\newcommand{\NumLatentFactors}{J}
\newcommand{\numlatentfactors}{j}
\newcommand{\NumComponents}{K}
\newcommand{\numcomponents}{k}




\received{2018 XX XX}
\revised{2018 XX XX}
\accepted{2018 XX XX}

\newcommand{\vcpath}{vc.tex}

\IfFileExists{\vcpath}{\input{\vcpath}}{
	\newcommand{\giturl}{UNKNOWN}
	\newcommand{\gitslug}{UNKNOWN}
	\newcommand{\githash}{UNKNOWN}
	\newcommand{\gitdate}{UNKNOWN}
	\newcommand{\gitauthor}{UNKNOWN}
}



\submitjournal{AAS Journals}

\shorttitle{A novel approach to chemical tagging in lower-dimensional latent space}
\shortauthors{Casey et al.}

\begin{document}

\title{A novel approach to chemical tagging in lower-dimensional latent space}

\correspondingauthor{Andrew R. Casey}
\email{andrew.casey@monash.edu}

\author[0000-0003-0174-0564]{Andrew R. Casey}
\affiliation{School of Physics \& Astronomy, 
			 Monash University,
			 Wellington Rd, Clayton 3800, Victoria, Australia}
\affiliation{Faculty of Information Technology, 
			 Monash University, 
			 Wellington Rd, Clayton 3800, Victoria, Australia}
			 
\author{John Lattanzio}
\affiliation{School of Physics \& Astronomy, 
			 Monash University,
			 Wellington Rd, Clayton 3800, Victoria, Australia}

\author{Aldeida Aleti}
\affiliation{Faculty of Information Technology, 
			 Monash University, 
			 Wellington Rd, Clayton 3800, Victoria, Australia}

\author{David Dowe}
\affiliation{Faculty of Information Technology, 
			 Monash University, 
			 Wellington Rd, Clayton 3800, Victoria, Australia}

\author{the GALAH team}


\begin{abstract}
Chemical tagging promises to distinguish unique star formation sites based on the
present day photospheric abundances of stars. Clustering techniques used to date
have treated all chemical abundances as independent, which we know is not true.
Nucleosynthetic processes make multiple elements in varying quantities, and the
dimensionality of chemical abundance space is typically lower than the reported
number of abundances. Here we introduce a novel approach to chemical tagging 
where a set of latent factors (e.g., nucleosynthetic yields) contribute to the
data, and there is clustering in the latent space (e.g., relative counts of
nucleosynthetic events). This approach allows us to infer contributions akin to
nucleosynthetic yields, and the clustering becomes more efficient as it occurs in
a lower dimensional latent space.
We run experiments on literature chemical abundances,
including the second \Galah\ data release. We find that 5 latent factors are
preferred to explain the chemical abundances of metal-poor stars, and 8 are
needed to account for \Galah\ data.
\todo{something about the clusters or factors found}
We release code associated with this \article\ that scales well with the 
available data: enabling chemical tagging for $10^{6}$ stars on a standard
desktop machine in minutes.
\end{abstract}


\keywords{methods: statistical}

\section{Introduction} \label{sec:intro}

Chemical tagging seeks to identify star formation events using the present
day photospheric abundances of stars \citep{Freeman;Bland-Hawthorn:2002}.
The detailed chemical abundances observable in a star's photosphere provide a
fossil record that carries with it information about where and when a star
formed. While the photospheric abundances remain largely unchanged throughout
a star's lifetime \citep[however see][]{Dotter:2017,Ness:2018b}, the dynamical 
dissipation timescale of open clusters in the Milky Way disc is of order a few 
gigayears \citep{Portegies-Zwart:1998}. That makes chemical tagging an attractive 
approach to identify star formation sites long after those stars are no longer 
gravitationally bound.


Gravitationally bound star clusters have been useful laboratories for
testing the limits and utility of chemical tagging. Although biases arise when
only considering star clusters that are still gravitationally bound, the chemical
homogenity of open clusters provides an empirical measure of how similar stars
would need to be before they could be tagged as belonging to the same
star formation site \citep{Mitschang:2014}. However, there are also analysis
issues in understanding how well those chemical abundances can be measured
\citep{Bovy:2016}. If open clusters were truely chemically homogeneous then our
ability to chemically tag the Milky Way depends on the precision with which we
can measure those chemical abundances in stars. Data-driven techniques are
driving a revolution in this field \citep{Ness:2015,Ness:2018a,Ness:2018b,
Casey:2016,Casey:2017,Ho:2017b,Ho:2017a,Leung;Bovy:2018}, but there is more work
to do: astronomers have not yet reached the Cram\'er-Rao bound for chemical 
abundances of stars.


There are further chemical tagging obstacles to overcome once all the chemical 
abundance information in a stellar spectrum can be extracted. Most notably is a 
catalogue of precise detailed chemical abundances for a large number of stars,
where those chemical abundances trace different nucleosynthetic pathways.
This is the primary scientific goal of the \Galah\ survey \citep{DeSilva:2015,Martell:2017,Buder:2018},
a stellar spectroscopic survey that uses the High Efficiency and Resolution 
Multi-Element Spectrograph \citep[\project{HERMES};][]{Sheinis:2015} on the Australian 
Astronomical Telescope (AAT).  \Galah\ will observe up to $10^6$ stars in the 
Milky Way, and measure up to 30 chemical abundances for each star. This includes
light odd-Z elements (e.g., Na, K), elements produced through
alpha-particle capture (e.g., Mg, Ca, Ti), and elements produced
through the slow (e.g., Ba) and rapid neutron-capture process
(e.g., Eu). These data provide an unparalleled view on the production
of chemical elements in the Milky Way.


Given these data and the most
favourable assumptions in chemical tagging -- that star clusters are truely
chemically homogenous, and we can measure those abundances with infinite precision,
and those abundances are differentiable between star clusters -- then chemical
tagging becomes a clustering problem. All clustering techniques applied to 
chemical tagging assume that the data dimensions are independent. That is to say
that adding a dimension of [Ni/H] provides totally new, independent information.
Theory and observations agree that this cannot be true.
Nucleosynthetic processes produce multiple elements in varying
quantities, and the dimensionality of stellar abundance datasets has been shown
to be lower than the actual number of dimensions \citep{Ting:2012, Price-Jones:2018}.
Any clustering approach that treats each new elemental abundance as an 
indenpendent axis of information will therefore conclude with biased inferences
about the star formation history of our Galaxy. 


Alternative approaches are perhaps more challenging, as it is difficult to prescribe
the nucleosynthetic yields that have contributed to the chemical abundances of each star
with great confidence. There are
qualitative statements that can be made for large numbers of stars, or particular
types of stars, but quantifying the precise contribution of different processes
to each star is an unsolved problem. For example, the [$\alpha$/Fe] `knee' in
abundance ratios in the Milky Way can qualitatively be explained by 
core-collapse supernovae being the predominant nucleosynthetic process in the
early Milky Way before Type Ia contributions had a significant impact, but 
efforts to date have not sought to try to explain the detailed abundances of a 
single star as a contribution of yields from different systems (e.g., 20\% of the
Ba was produced by 3 M$_\odot$ systems). This is in part because of the
challenging and degenerate nature of the problem as described, and is complicated
by the differences in yield predictions that account from prescriptions used in
different codes.


New approaches to chemical tagging are clearly needed. Immediate advances would
include methods that take the dependence among chemical elements into account
within some generative model, or techniques that combine chemical abundances
with dynamical constraints to place joint prior probabilities on whether any
two stars could have formed from the same star cluster, given some model of the
Milky Way. In this work we focus on the former. Here we present a new approach
to chemical tagging that allows us to identify the latent (unobserved) factors
that contribute to the chemical abundances of all stars (e.g., nucleosythetic
yields) while simultaneously performing clustering in the latent space.
Notwithstanding caveats that we will
discuss in detail, this allows us to infer nucleosynthetic yields rather than
strictly prescribe them from models. Moreover, the scale of the clustering
problem reduces by a significant fraction because the clustering is performed in
a lower dimensional latent space instead of the higher dimensional data space.
In Section~\ref{sec:methods} we describe the model and the methods we use to
estimate the model parameters. Section~\ref{sec:experiments} describe the 
experiments performed using generated and real data sets. We discuss the results
of these experiments in Section~\ref{sec:discussion}, including the caveats with
the model as described. We conclude in Section~\ref{sec:conclusion}.



\section{Methods} \label{sec:methods}

Factor analysis is a common statistical approach for describing correlated 
observations with a lower number of latent variables \citep[e.g.,][]{Thompson:2004}.
Related techniques include principal component analysis \citep{Hotelling:1933} and its
variants \citep{Tipping;Bishop:1999}, singular value decomposition \citep{Golub:1970}, and other
matrix factorization methods. While factor analysis on its own is a useful
dimensionality reduction tool to identify latent factors that contribute to
the chemical abundances of stars \citep[e.g.,][]{Price-Jones:2018}, it does not perform any clustering.
Similarly, clustering techniques applied to chemical abundances to date 
\citep[e.g.,][]{Hogg:2016} do not account for the lower dimensionality of the
data. 

Here we use a variant of factor analysis known elsewhere as a mixture of common 
factor analyzers \citep{Baek:2010}, where the data are generated by a set of 
latent factors that are common to all data, but the scoring (or extent) of those
factors is different for each data point, and the data can be modelled as a
mixture of multivariate normal distributions in the latent space (factor scores).
In this work the data $\vecdata$ is a 
$\NumData \times \NumDimensions$ matrix where $\NumData$ is the number of 
stars and $\NumDimensions$ is the number of chemical abundances measured 
for each star. We assume a generative model for the data 
\begin{equation}
	\vecdata = \factorloads\factorscores + \vec{e}
	\label{eq:generative-model}
\end{equation}

\noindent{}where $\factorloads$ is a $\NumLatentFactors \times \NumDimensions$ 
matrix of factor loads that is common to all data points, and the factor scores 
for the $\numdata$th data point
\begin{equation}
	\factorscores_\numdata \sim \mathcal{N}(\vec\xi_\numcomponents, \vec\Omega_\numcomponents)
\end{equation}
\noindent{}are drawn from the $\numcomponents$th multivariate normal distribution.
The factor scores for all data points $\factorscores$ is then a 
$\NumData \times \NumLatentFactors$ matrix, where each data point has a partial
association to the components in latent space. 
We assume $\vec{e} \sim \mathcal{N}\left(\vec{0}, \eye\specificvariance\right)$
is independent of the latent space, and $\specificvariance$ is a
diagonal matrix of $\NumDimensions$ entries. 
%In Appendix~\ref{app:symbols}
%we list all mathematical symbols used in this work and their description.
In this model each data point can be represented as being drawn
from a mixture of multivariate normal components, except the components
are \emph{clustered in the latent space} $\factorscores$ and projected
into the data space by the factor loads $\factorloads$. 


We assume that the latent space is lower dimensionality than the
data space (e.g., $\NumLatentFactors < \NumDimensions$).
Within the context of stellar abundances, the factor loads
$\factorloads$ can be thought of as the \emph{typical} yields
of nucleosynthetic
events (e.g., $s$-process production from AGB stars), and the
factor scores are analogous to the relative counts of those 
nucleosynthetic events. The clustering in factor scores
achieves the same as a clustering procedure in data space,
except we simultaneously estimate the latent processes that are
common to all stars (the so-called factor loads, analogous to 
nucleosynthetic yields). Within this framework a rare nucleosynthetic event
can still be described as a `factor load' $\factorloads_\numlatentfactors$, 
but its rarity would be represented by associated factor
scores being zero for most stars and thus have negligible contribution
to the observed abundances. In practice the factor loads can only be 
identified up to orthogonality and cannot be expressly interpreted as
nucleosynthetic yields because they have limited physical meaning,
but this description of typical yields and relative event rates should
help build intuition for the model parameters, and provide context
within the astrophysical problem it is being applied.


Including latent factors in the model description allows us to account for 
processes that affect multiple elemental abundances. In this way we are 
accounting for the fact that the data dimensions are not all independent of
each other. Another benefit is the scaling with computational cost. If we 
considered data sets of order $3\times10^7$
entries (e.g., 30 chemical abundances for $10^6$ stars) purely as a
clustering problem, then even the most efficient clustering
algorithms would incur a significant cumulative computational 
overhead by searching the parameter space for the number of
clusters, and the optimal model parameters given that number
of components. However, because the mixture of factor analyzers
approach assumes that there is a \emph{lower dimensional latent 
space} in which the data are clustered, and that clustering is 
projected into real space by common factor loads, the 
dimensionality of the clustering problem is reduced from 
$N \times D$ to $N \times J$. This reduces computational cost through
faster execution of each E-M step, and on average fewer E-M steps
needed to reach a specified convergence threshold.

From a statistical standpoint, the primary advantage to using
a mixture of factor analysers is that we can simultaneously
estimate latent factors (e.g., infer nucleosynthetic 
yields) and perform clustering (e.g., chemical tagging) 
within a statistically consistent framework. That is to say
that we have a generative model for the data that can 
quantitatively account for nucleosynthetic yields, variations in
turbulence and gas mixing, or star formation efficiency,
%can account
%analogues of nucleosynthetic yields and star formation,
and the parameters of this model can be estimated consistently
given some data.

Without loss of generality the density of the data $\vecdata$ can be described as
\begin{equation}
	f(\vecdata; \vec\Psi) = \sum_{\numcomponents=1}^{\NumComponents}\weight_\numcomponents\phi(\vecdata;\factorloads\scoremeans_\numcomponents, \factorloads\scorecovs_\numcomponents\factorloads\transpose + \eye\specificvariance)
\end{equation}
\noindent{}given $\NumLatentFactors$ common factor loadings and $\NumComponents$ components
clustered in the latent (factor score) space. Here the parameter
vector
$\vec\Psi$ includes $\{\factorloads,\vec\pi,\scoremeans,\scorecovs,\specificvariance\}$, and $\phi(\vecdata;\vec\mu, \vec\Sigma)$
describes the density of a multivariate gaussian distribution with
mean $\vec\mu$ and covariance matrix $\vec\Sigma$,
and $\weight_\numcomponents$ describes the relative weighting of the $\numcomponents$th
component in latent space and $\sum\weight_\numcomponents = 1$.
The log likelihood is then given by
\begin{equation}
	\log\mathcal{L}(\vecdata|\vec\Psi) = \sum_{\numcomponents=1}^{\NumComponents}\log{f(\vecdata;\vec\Psi)} \quad .
\end{equation}


As mentioned previously, the model as described is indeterminate in that
there is no unique solution for the factor loads $\factorloads$ and scores
$\factorscores$. These quantities can only be determined up until 
orthogonality. However, accurate estimates of the parameter vector $\vec\Psi$
can be obtained by the expectation-maximization algorithm \citep{Dempster:1977}.
%The model described by Equation~\ref{eq:generative-model} is indeterminate:
%there is no unique solution for the factor loads $\factorloads$ and scores
%$\factorscores$. However, an accurate estimate of the parameter vector $\vec\Psi$ can be obtained by the expectation-maximization algorithm \citep{EM}. 



\subsection{Initialisation}

Here we describe the default initialisation of the model parameters, which
we have found to be robust in a large set of models. Given a number of
$\NumLatentFactors$ latent factors we initialise the factor load
$\factorloads$ entries to be drawn from a uniform distribution
$\factorloads \sim \mathcal{U}\left(-1, +1\right)$ and then ensure that
the factor loads are orthogonal such that
%TODO: Should we re-write this so that we just draw from the scipy special ortho group?
\begin{equation}
	\factorloads \factorloads\transpose = \eye \quad .
\end{equation}
%        AL = linalg.cholesky(A.T @ A)
%        A = A @ linalg.solve(AL, np.eye(self.n_latent_factors))
% 	
We then initially assign each data point as belonging to one of the
$\NumComponents$ components by drawing from a multinomial distribution
and generating the $\NumData \times \NumComponents$ responsibility matrix $\vec\tau$,
where $\sum_{k=1}^{K} \tau_{nk} = 1$ for any $n$th row. Given the initial
factor loads and assignments, we then estimate the relative weights
$\vec\pi$, the mean factor scores of each component $\scoremeans$, and
the covariance matrix of factor scores of each component $\scorecovs$.
Finally, we initialise the specific variance $\specificvariance$ in each
dimension as 1. 

Other initialisation methods are available in the code 
associated with this \emph{Article}. In later Sections we show that 
the optimised model parameters are insensitive to the choice initialisation, 
up until orthogonal rotation of the latent space.


\subsection{Expectation-Maximization}

We use the expectation-maximization algorithm to estimate the model parameters
\citep{Dempster:1977}. With each expectation step we evaluate the log likelihood 
given the model parameters $\vec\Psi$, and we re-calculate the responsibility 
matrix $\vec\tau$ whose entries are the posterior probability that the 
$\numdata$th data point is associated to the $\numcomponents$th component, given 
the data $\vecdata$ and the current estimate of the parameter vector $\vec\Psi$:
\begin{equation}
	\tau_{\numdata\numcomponents} = \frac{\weight_\numcomponents\phi(\vecdata_\numdata;\factorloads\scoremeans_\numcomponents, \factorloads\scorecovs_\numcomponents\factorloads\transpose + \eye\specificvariance)}{\sum_{g=1}^{G}\weight_g\phi(\vecdata_\numdata;\factorloads\scoremeans_g, \factorloads\scorecovs_g\factorloads\transpose + \eye\specificvariance)} \quad .
\end{equation}


At the maximization step we update our estimates of the parameters,
conditioned on the data $\vecdata$ and the responsibility matrix $\vec\tau$.
First we estimate the relative weights $\vec\weight\nextstep$ given
the responsibility matrix $\vec\tau$
\begin{equation}
	\weight_\numcomponents\nextstep = \frac{1}{\NumData} \sum_{\numdata=1}^{\NumData}\tau_{\numdata\numcomponents}
	% Include proof.
\end{equation}
\noindent{}where the $\vec{X}\thisstep$ superscript refers to the current estimate of a variable
and $\vec{X}\nextstep$ refers to the updated estimate for the next iteration.
%\footnote{Here $\vec\textrm{X}\now$ refers to the current estimate of a variable $\vec\textrm{X}$ and $\vec\textrm{X}\nextstep$ refers to the updated estimate for the next iteration.}


%\noindent{}In the equations that follow (Equation~\ref{eq:X} to \ref{eq:Y})
%when we refer to any value in $\{\factorloads,\vec\pi,\scoremeans,\scorecovs,\specificvariance\}$ for brevity we are referring to the current estimate of that
%parameter (e.g., $\factorloads\thisstep$). The updated estimates are marked as such
%(e.g., $\scoremeans\nextstep$). 

The updated estimates of the mean factor scores 
$\scoremeans\nextstep$ for each component are then given by
\begin{eqnarray}
	\scoremeans_\numcomponents\nextstep = \scoremeans_\numcomponents\thisstep + \frac{\vec{G}\transpose(\vecdata\transpose - \factorloads\thisstep\scoremeans_\numcomponents\thisstep)\vec\tau_\numcomponents}{\NumData\weight_\numcomponents\nextstep}
\end{eqnarray}
\noindent{}where:
\begin{eqnarray}
	\vec{W} &=& (\scorecovs_\numcomponents\thisstep)^{-1}\eye \\
	\vec{V} &=& \left(\specificvariance\thisstep\right)^{-1} \\
	\vec{C} &=& (\vec{W} + (\factorloads\thisstep)\transpose\vec{V}\factorloads\thisstep)^{-1}\eye \\
	\vec{G} &=& \left[\vec{V} - \vec{V}\factorloads\thisstep\vec{C}\left(\vec{V}\factorloads\thisstep\right)\transpose\right]\factorloads\thisstep\scorecovs_k\thisstep \quad .
\end{eqnarray}

The covariance matrices of the components of factor scores $\scorecovs\nextstep$
are updated next,
\begin{equation}
	\scorecovs_\numcomponents\nextstep = \left(\eye - \vec{G}\transpose\factorloads\thisstep\right)\scorecovs_\numcomponents\thisstep + \frac{\vec{G}\transpose\vec{Z}\left(\vec{Z}\vec\tau_\numcomponents\transpose\right)\transpose\vec{G}}{N\weight_\numcomponents\nextstep}
\end{equation}
\noindent{}where
\begin{eqnarray}
	\vec{Z} &=& \vecdata\transpose - \factorloads\thisstep\scoremeans_\numcomponents\thisstep \quad .
\end{eqnarray}

After some linear algebra, updated estimates of the common factor loads $\factorloads\nextstep$
can be found from
\begin{equation}
	\factorloads\nextstep = \factorloads_{1}\left(\factorloads_{2}^{-1}\eye\right)
\end{equation}
\noindent{}where:
\begin{eqnarray}
	\factorloads_1 &=& \sum_{\numcomponents=1}^{\NumComponents}\left[ \vec\tau_\numcomponents\transpose\vecdata\left(\scoremeans_\numcomponents\thisstep\right)\transpose + 
	\vec{G}\transpose\vec\tau_\numcomponents\vec{Z}\transpose\vec{G}\right] \\
	%\left(\scoremeans_k\thisstep\vecdata\transpose\vec\tau_k\right)\transpose \left(\vec{G}\transpose\vec{Z}\vec\tau_k\transpose\vec{G}\right)\transpose\right]  \\
	\factorloads_2 &=& N\sum_{\numcomponents=1}^{\NumComponents}\left[\weight_\numcomponents\nextstep\left(\scorecovs_\numcomponents\nextstep + \scoremeans_\numcomponents\nextstep\left(\scoremeans_\numcomponents\nextstep\right)\transpose\right)\right]
\end{eqnarray}


Finally, the updated estimate of the specific variances $\specificvariance\nextstep$ is given
by
\begin{equation}
	\specificvariance\nextstep = \frac{1}{\NumData}\left[\sum^{\NumComponents}_{\numcomponents=1}\vec\tau_\numcomponents\transpose\left(\vecdata\odot\vecdata\right) - \sum_{j=1}^{J}\left(\factorloads\nextstep\factorloads_2\right)\odot\factorloads\nextstep\right]
\end{equation}

\noindent{}where $\odot$ denotes is the entry-wise (Hadamard) product. We repeat
the expectation-maximization cycle for up to 10,000 steps or until the log-likelihood
improved by less than $10^{-5}$ between successive iterations. Although our implementation
allows for a positive regularisation term to be added along the diagonal of the covariance
matrices in latent space $\scorecovs$, we set this regularisation term to zero for all
experiments performed here.


\section{Experiments} \label{sec:experiments}


\subsection{Experiment 1: Toy model with generated data} \label{sec:experiment-toy-model}

We generated a data set with ${\NumData = 100000}$ data points, each with
$\NumDimensions = 15$ dimensions, and assumed that those data are generated by
latent space of $\NumLatentFactors = 5$ ($\NumLatentFactors \times \NumDimensions$) factor loads, and there are $\NumComponents = 20$
clusters in the latent space. The relative weights $\vec\weight$
are drawn from a multinomial distribution and the means of the clusters
in factor scores $\scoremeans$ are drawn from a standard normal
distribution. The covariance matrices in factor scores $\scorecovs$ are assumed to be diagonal matrices, where the non-zero entries are drawn from a gamma distribution $\scorecovs_{\numcomponents,i,i} \sim \vec\Gamma\left(1\right)$. The variance in 
each dimension $\specificvariance$ are also drawn $\specificvariance \sim \vec\Gamma\left(1\right)$.
The $\numdata$th data point (which belongs to the $\numcomponents$th cluster) is then
generated by drawing $\factorscores_{\numdata} \sim \mathcal{N}(\scoremeans_\numcomponents,\scorecovs_\numcomponents)$, projecting by the factor loads, and adding variance.



\begin{figure*}
	\includegraphics[width=1.0\textwidth]{experiments/exp1-data.png}
    \caption{A corner plot showing all $\NumDimensions$ dimensions of the
    		 generated data for Experiment~1 (blue). Overplotted in red
		 	 we show samples generated given our estimate of the model 
			 parameters, without adding specific variance in each dimension.
			 This demonstrates that the model parameters estimated by
			 expectation-maximization can replicate the generated data.}
    \label{fig:exp1-data}
\end{figure*}


%\begin{figure*}
%	\includegraphics[width=1.0\textwidth]{experiments/toy-model-latent-space.png}
%    \caption{A corner plot showing all $\NumLatentFactors$ dimensions in
%    		 latent space, where each data point is coloured by the component
%		 	 that it is estimated to belong to.}
%    \label{fig:toy-model-latent-space}
%\end{figure*}


%\subsubsection{Given the true number of latent factors and components}

%We initialised the factor loads and component assignments randomly, and
%fit this generated data set using the expectation-maximization algorithm
%as described in the previous Section. After \todo{58} iterations
%(and a serial processing time of \todo{40} seconds) we
%reached our convergence threshold: the log probability increased by
%less than $10^{-5}$ with each iteration (Figure~\ref{fig:exp1-ll-iterations}). For this experiment we specified
%the number of latent factors $\NumLatentFactors$ and the number of clustering components
%$\NumComponents$ to include, but below we repeat this experiment where the number
%of latent factors and clustering components is not known.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{experiments/exp1-ll-iterations.png}
	\caption{Log-likelihood of the model given the data $\mathcal{L}\left(\vec\data|\vec\Psi\right)$ for Experiment 1 as a function of the number of expectation-maximization iterations. After \todo{X} iterations the improvement in log-likelihood per
    expectation-maximization cycle was less than $10^{-5}$.}
    \label{fig:exp1-ll-iterations}
\end{figure}



%While the estimated model parameters can generate the data and demonstrate
%%clustering in latent space, caution is needed when interpreting the 
%model latent variables.
%Latent factor models are by definition indeterminate in that there are
%multiple solutions of the parameter vector $\vec\Psi$ which provide
%an identical data-generating process. This can be trivially shown by
%the fact that the factors loads and scores can be rotated by taking the
%dot product
%with some rotation
%matrix $\textbf{R}$ and generate the exact same data. 
%We discuss how this limitation applies to later experiments.

%This is further complicated by orientation and order
%of the latent factors: even if the inferred factor loads were very close
%to the true factor loads, they could be ordered differently, or have
%their signs reversed. We discuss this further in later sections.

%\subsubsection{Grid search for the number of latent factors and components}

Here we treat the generated data set as if the true number of latent factors
and the true number of components are not known. Starting with $\NumLatentFactors = 1$
and $\NumComponents = 1$, we trialled each permutation of $\NumLatentFactors$ and $\NumComponents$
until $\NumLatentFactors_{max} = 10$
and   $\NumComponents_{max} = 40$ (e.g., twice $\NumLatentFactors_\textrm{true}$ and $\NumComponents_\textrm{true}$).
For each $\NumLatentFactors,\NumComponents$ permutation we initialised the factor
loads and component assignments randomly. We performed expectation-maximization 
cycles until the relative log-likelihood improved by less than $10^{-5}$. We then
recorded the log-likelihood of the data given the model parameters, and the
Bayesian Information Criterion \citep[BIC][]{Schwarz:1978}, 
\begin{equation}
	\textrm{BIC} = Q\log{N} - 2\log\mathcal{L}\left(\data|\vec\Psi\right) \quad , \label{eq:bic}
\end{equation} 
\noindent{}where $Q$ is the number of model parameters:
\begin{equation}
	%Q = \NumComponents + \NumDimensions - 1 + \NumLatentFactors(\NumDimensions + \NumComponents) + \frac{1}{2}\NumComponents\NumLatentFactors(\NumLatentFactors+1) - \NumLatentFactors^2 \quad .
	Q = \frac{\NumLatentFactors}{2}\left[2\left(\NumDimensions - \NumLatentFactors\right) + \NumComponents\left(3 + \NumLatentFactors\right)\right] + \NumComponents + \NumDimensions - 1
\end{equation}

These metrics are shown in Figure~\ref{fig:experiment-1-gridsearch}.
Unsurprisingly the log likelihood increases with increasing numbers of latent
factors $\NumLatentFactors$ and increasing numbers of components $\NumComponents$.
The lowest BIC value is found at $\NumLatentFactors = 5$
and $\NumComponents = 21$, close to the true values ($\NumLatentFactors_\textrm{true} = 5$,
$\NumComponents_\textrm{true} = 20$). 

We show a corner plot of the data in Figure~\ref{fig:exp1-data}.
In this figure we have over-plotted 100000 samples drawn from the model
with $J = 5$ and $K = 21$
-- without adding variance $\specificvariance_\textrm{est}$ % Ref Factor load figure 
-- in order to illustrate that the
estimated model parameters can realistically generate the data.
The estimated latent space of factor scores is shown in 
Figure~\ref{fig:exp1-latent-space} where the colours indicate the
most likely component that the data point is associated with.


%\noindent{}and the pseudo-Bayesian Information Criterion \citep{pseudo-bic}
%\begin{equation}
% \textrm{pseudo-BIC} = 6(1 + \gamma)\omega{}Q\log{N} - 2\log\mathcal{L}\left(\data|\vec\Psi\right) \label{eq:pseudo-bic}
%\end{equation}
%\noindent{}where $Q$ is the total number of model parameters in both equations,
%and the unknown constants $\omega$ and $\gamma$ have constraints
%$\omega \geq 1$ and $\gamma > 0$. Here we adopt $\gamma = 0.1$ and $\omega = 1$.



\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp1-gridsearch-ll-contours.png}
	\includegraphics[width=0.45\textwidth]{experiments/exp1-gridsearch-bic-contours.png}
    \caption{Model performance metrics resulting from a gridsearch using the
    		 data generated as part of our toy model. The top 
		 	 panel shows the negative log-likelihood 
			 $-\log{\mathcal{L}\left(\data|\vec\Psi\right)}$ 
			 evaluated at each combination of latent factors $J$ and number 
			 of clusters $K$, the middle panel shows the BIC (Equation \ref{eq:bic}), 
			 and the lower panel shows
			 the pseudo-BIC
			 (Equation~\ref{eq:pseudo-bic}). The marker indicates the 
			 lowest value in each panel.}
    \label{fig:experiment-1-gridsearch}
\end{figure}




%\begin{figure*}
%	\includegraphics[width=1.0\textwidth]{experiments/toy-factors-init-randomly.png}
%	\caption{Latent factor loads inferred for models with increasing numbers
%			 of latent factors and increasing numbers of cluster components
%			 in Experiment~1, where the latent factors and component assignments
%			 were initialised randomly.
%				The top left panel shows the results of the model with
%			 $\NumLatentFactors = 1$ and $\NumComponents = 1$. 
%			 The latent factors have each been rotated by a
%			 rotation matrix such 
%			 that the estimated latent factors in each model are ordered 
%			 and oriented to be as similar as the latent factors estimated from
%			 the model with only one latent factor, up to orthogonality such
%			 that $\vec{R}\vec{R}\transpose = \eye$.
%			 }
%	\label{fig:toy-factors-init-randomly}
%\end{figure*}

%\todo{This structure goes away and becomes more consistent if we change the initialisation procedure}

%\todo{figure of toy factors init svd kmeans++}

%\todo{log-likelihood of different initialisation routines}



\begin{comment}
The results of this experiment confirm that the model is indeterminate, in 
that there are multiple equally valid solutions that can exactly reproduce
the data. It also highlights the importance of initialisation, demonstrating
how the initialisation of the common latent factors can impact the final
model parameters. These two points are related, as one could initialise the
latent factors randomly (or from noise) and multiply that model with a
rotation matrix $\mathbf{R}$ that rotates the latent space such that the
common factor loads are aligned closer to some prior expectation that we have
for the common factor loads (e.g., in that they should represent plausible
astrophysical yields). Motivated by these results and indeterminate nature
of the model, in Experiment~2 we initialise the common factor loads
to represent known astrophysical processes such that they can be interpreted
in context with the data.


\subsection{Experiment 2: A minimal set of nucleosynthetic trace elements in \Galah}

In this experiment we further investigate the effects of initialising the model parameters.
Here we make use of a minimal set of nucleosynthetic trace elements
using the second \Galah\ data release \citep{Buder:2018}. Specifically, we use
reliable chemical abundance measurements of Na (an odd-Z element), Mg and Ca (alpha-elements),
Ni, Fe, and Cr (iron-peak elements), Ba and Y (s-process tracers), and Eu (an r-process tracer).
We assume that there are four latent factors ($J = 4$) and four components ($K = 4$),
and we initialise the latent factors and component parameters in different combinations.

To initialise the latent factors using astrophysical insight (or domain expertise)
we make the following steps:
\begin{enumerate}
	\item We initialise all entries of $\factorloads$ to be drawn from $\mathcal{N}\left(0, 10^{-2}\right)$.
	\item For the first latent factor we set the entries that correspond to Ni, Fe, and Cr to be $+1$ (e.g., Fe-peak).
	\item For the second latent factor we set the entries that correspond to Mg and Ca to be $+1$ (e.g., $\alpha$-elements).
	\item For the third latent factor we set the entries that correspond to Ba and Y to be $+1$ (e.g., the s-process).
	\item For the fourth latent factor we set the entry that corresponds to Eu to be $+1$ (e.g., the r-process).
\end{enumerate}


%The latent factors can be initialised in four different ways: (1) with entries drawn randomly from $\mathcal{U}\left(-1, +1\right)$, (2) with noisey draws $\mathcal{N}\left(0, 10^{-2}\right)$, (3) using singular value decomposition, and (4) using astrophysical insight to initialise the factor loads. 

The component parameters are either initialised randomly, or using \texttt{K-means++} algorithm in
the latent space given the initial estimate of the latent factors. For each combination of 
initialisation procedure, we run 30 optimisations and record the log likelihood once the E-M 
procedure has reached a threshold of $10^{-8}$.
The results of this experiment are shown in Figure~\ref{fig:experiment-4-log-L}.
In most cases the final log likelihood is within the threshold of $10^{-8}$ between
different optimisation runs of the same initialisation procedure. However, it is clear
that at least some initialisations result in a local optimum, emphasising the importance in
optimising from many initialisations. 

Figure~\ref{fig:experiment-4-log-L} demonstrates that even though the initialisation procedure
can impact the final estimate of the model parameters, the resulting log likelihood values
are within the convergence threshold. This is, again, a consequence of the indeterminate
nature of the model specification. However, the advantage here is that these results
demonstrate that we \emph{can} initialise the latent factors with domain knowledge,
without a significant detriment to the final log likelihood. Similarly, in Figure~\ref{fig:experiment-4-astrophysical-latent-factors}
we show that the initial latent factors are indeed different from the final latent factors,
and that there is some variance in our estimates of the latent factors from 30 initialisations.
\todo{something to say here about not biasing our results?}

The results from this experiment demonstrate that we can initialise the latent factors
using domain knowledge without incurring strong biases in our final estimates of the
latent factors, or introducing a significant detriment to the log likelihood, as long
as we initialise the model many (e.g., $\gtrsim 5$) times.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{experiments/galah-experiment-4-log-L-hist.png}
	\caption{Log likelihoods $\mathcal{L}\left(\data|\vec\Psi\right)$ of optimised
			 models using \Galah\ data in Experiment 4 with four latent factors
			 and four components ($J = 4$, $K = 4$). The columns indicate different
			 methods to initialise the component parameters ($\pi$, $\scoremeans$,
			 $\scorecovs$), and the rows represent different methods to initialise
			 the latent factors ($\factorloads$). Each combination of initialisation
			 procedure has 30 models where E-M was run until a threshold of $10^{-8}$
			 was reached in $\log{L}$.}
	\label{fig:galah-experiment-4-log-L}
\end{figure}


\todo{experiment-4-astrophysical-latent-factors}
\end{comment}


\subsection{Experiment~2:\\Inferring astrophysically realistic latent factors}

Latent factors can only be identified up to orthogonality. That is to say that
if the data were truely generated by latent factors $\factorloads_\textrm{true}$,
then our estimates of those latent factors $\factorloads_\textrm{est}$ may not
be identical to the true values. For example, the ordering of the estimated factors
could be different from the true factors, and the ordering of the dimensionality
in latent space would be correspondingly different. Another possibility is that
the estimated factor loads could be flipped in sign relative to the true factor
loads, and the scores would similarly be flipped. In both of these situations
(re-ordering or flipped signs) the log likelihood given the data and the
estimated factor loads $\factorloads_\textrm{est}$ would be identical to the
log likelihood given the data and the true factor loads $\factorloads_\textrm{true}$
despite the difference in ordering and sign. These examples illustrate a more 
general property that the factor loads and factor scores can be orthogonally 
rotated by any rotation matrix\footnote{A rotation matrix is valid if 
$\vec{R}\vec{R}\transpose = \vec{I}$} $\vec{R}$. The estimated factor loads 
$\factorloads_\textrm{est}$ could therefore appear very different from the true 
values, but they only differ by an orthogonal rotation. 


If the true values are known then it is trivial to find a rotation matrix $\vec{R}$
that rotates the estimated factor loads to the true factor loads. However, when
the true values are not known, this property of factor analysis limits the
extent to which we can interpret the latent variables in our model. For example,
if we seek to interpret the factor loads as being something analagous to 
nucleosynthetic processes, then the estimated factor loads will need to be 
rotated to something that looks astrophysically plausible before we could 
attribute one latent factor as being a particular nucleosynthetic process. 
Although having to perform this rotation is not ideal, there is still utility in
trying to identify the latent factors because the factors can only be rotated by
some valid rotation matrix. That is to say that we will not only exactly what
we have put in: the rotated factors will simply be the closest set of factor
loads that are consistent with the data.


In this experiment we use detailed chemical abundances of metal-poor stars as
reported by \citet{Barklem:2005}, and we demonstrate the utility and limitations
in interpreting the latent factors as nucleosynthetic processes. The data 
contains 14 measurements of chemical abundances for 61 stars ($N = 61$, $D = 14$): 
Al (light odd-Z), Mg and Ca ($\alpha$), Sc, Ti, Cr, Mn, Fe, Co, and Ni (Fe-peak),
as well as Sr, Y, Ba, and Eu ($s$/$r$-process). We performed a grid search for
the optimal number of latent factors $J$ and components $K$, starting from
$J = 1$ and $K = 1$, and trialling up until $J = 8$ and $K = 5$. We initialised
the model parameters randomly for each permutation of $J$ and $K$, and ran E-M
until a convergence threshold of $10^{-5}$ was reached in the log likelihood. 
In Figure~\ref{fig:exp2-gridsearch-contours} we show the negative log likelihood 
and BIC recorded for the optimised set of model parameters for each permutation 
of $J$ and $K$. The lowest BIC value is found for $J = 5$ and $K = 1$, indicating
five latent factors are preferred, and no clustering.

\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp2-gridsearch-ll.png}
	\includegraphics[width=0.45\textwidth]{experiments/exp2-gridsearch-bic.png}
    \caption{Model performance metrics resulting from a gridsearch using the
    		 \citet{Barklem:2005} data in Experiment~2.
    		 The top panel shows the negative log-likelihood 
			 $-\log{\mathcal{L}\left(\data|\vec\Psi\right)}$ 
			 evaluated at each combination of latent factors $J$ and number 
			 of clusters $K$, and the lower panel shows the BIC (Equation \ref{eq:bic}).
			 The cross at
			 $J=8$ and $K=5$ indicates that the model could not converge without
			 adding a regularisation constant to the diagonals of $\scorecovs$.
			 The marker indicates the lowest value in each panel, showing that
			 $J = 5$ and $K = 1$ is preferred by BIC.}
    \label{fig:exp2-gridsearch-contours}
\end{figure}


Using our preferred model with $J = 5$ and $K = 1$, we rotated the estimated
factor loads in order to be astrophysically plausible. Specifically we constructed
a $J \times D$ astrophysical factor load matrix $\factorloads_\textrm{A}$ where
all entries were zero except for the following entries:
\begin{itemize}
	\item the entry corresponding to Al in the first latent factor to represent
		  odd-Z element production,
	\item the entries corresponding to Ca and Mg in the second latent factor to
		  represent $\alpha$ particle production,
	\item the entries corresponding to Ni, Co, Fe, Mn, Cr, Ti, and Sc in the third
		  latent factor to represent the Fe-peak group,
	\item the entries corresponding to Sr, Y, and Ba in the fourth latent factor
		  to represent products of the slow neutron capture process, and
	\item the entries corresponding to Eu in the fifth latent factor to represent
		  products of the rapid nucleosynthesis process.
\end{itemize}
These entries were set to $1/\sqrt{E}$ where $E$ is the number of non-zero entries
in that latent factor, in order to ensure that $\factorloads_\textrm{A}$ is
orthonormal. We optimise the $J$ angles needed to make the estimated factor loads
$\factorloads_\textrm{est}$ as close to the astrophysical factor loads $\factorloads_\textrm{A}$.
In practice we take the product of many  Givens rotation matrices \citep{Givens},
which reduces to Euler angle rotation in three or fewer dimensions. In Figure
\ref{fig:exp2-factor-loads} we show the rotated factor loads found by our
preferred model, where the shaded regions indicate the projected uncertainty on
the rotation angles -- not including any uncertainty on the factor loads themselves.


The rotated loads are not precisely the same as the target astrophysical loads
in part because the target loads only seek to \emph{identify} and \emph{order}
factors that can be interpreted within an astrophysical context. For example,
the first factor does show positive contributions to Al, like the target load,
but it also shows a slight negative contribution with respect to alpha elements
(Mg, Ca), and a gradual increase among Fe-peak elements up to Ni. Similarly while
Ca and Mg are indeed positive in the second factor, as targeted, there is a
weak negative contribution to Mn abundances, which was not part of the target
loads. The fourth factor shows a positive contributions to Sr, Y, Ba, and to a
lesser extent, Eu, which is representative of slow neutron capture production.
Similarly, the last component shows a very strong positive contribution towards
Eu, and could be interpreted as production from the rapid nucleosynthesis process.


The relatively wide uncertainty regions on these factors is perhaps not
surprising given that only 61 stars were used in this data set. However, this
experiment hopefully demonstrates that the rotated latent factors have some
interpretability, albeit limited. After accounting for the contributions by all
$J = 5$ factors, the specific scatter remaining in each dimension is shown in
Figure~\ref{fig:exp2-specific-scatter}. The remaining scatter varies from as low 
as 0.01-0.02~dex (Mg, Ca, Ti, Mn, Fe, Y) up to 0.08-0.16~dex (Al and Ba). 


\begin{figure*}
	\includegraphics[width=\textwidth]{experiments/exp2-latent-factors.png}
	\caption{Rotated latent factors found in Experiment~2 using the \citet{Barklem:2005}
			 data. \todo{elaborate latent factors}}
    \label{fig:exp2-latent-factors}
\end{figure*}



\begin{figure}
	\includegraphics[width=0.45\textwidth]{experiments/exp2-specific-scatter.png}
	\caption{Specific scatter remaining in the \citet{Barklem:2005} data used in
			 Experiment~2 after accounting for the contributions by all latent
			 factors.}
    \label{fig:exp2-specific-scatter}
\end{figure}



\subsection{Experiment~3: A subset of gravitationally bound stellar clusters 
			from \Galah\ and elsewhere}
\label{sec:exp3}

The previous experiment has demonstrated that the latent factors can be at least
partially interpreted within the context of astrophysical processes. In this
experiment we seek to test whether gravitationally bound stellar clusters can be
reliably identified by their chemical abundances alone, and using only the model 
described in Section~\ref{sec:model}.

\todo{description  of the open cluster data used here}





\subsection{Experiment 4: Chemical abundances in the \Galah\ survey}
\label{sec:exp4}


We use the second \Galah\ data release \citep{Buder:2018a} which
includes up to 23 chemical abundances measured for 342,682
stars. It is not practical to use all of these measurements as-is
when performing chemical tagging. For example, our experiments
with \Galah\ data do not include lithium abundances because the
photospheric lithium varies throughout a star's lifetime. 
We first selected stars with \texttt{flag\_cannon = 0} to exclude
stars where there is reason to suspect that the stellar parameters
(e.g., $\teff$, $\logg$) are unreliable, and as a result the 
detailed chemical abundances are untrustworthy. 

We included the following fourteen chemical abundances: \todo{Na, Mg, 
Si, Ca, Sc, Ti, Cr, Mn, Fe, Ni, Cu, Zn, Ba, and Eu}.
By requiring that each star has \texttt{flag\_cannon = 0} and
\texttt{flag\_<x> = 0} (where \texttt{<x>} is the relevant label)
for each chemical abundance, this leaves
us with a sample of \todo{12,798} stars each with reliable stellar
parameters ($\teff$, $\logg$) and chemical abundances.
This is relevant to the approach we use here, as the method
described here requires no missing data.
These elements are produced from multiple nucleosynthetic
pathways, and it is this subsample that we will use for 
our initial chemical experiment with \Galah\ data.


\todo{we perform a grid-based search for J and K}



\subsection{Experiment 5: More chemical abundances in \Galah}
\label{sec:exp5}

\todo{which abundances, only require finite and \texttt{flag\_cannon} to be OK}

\section{Discussion} \label{sec:discussion}

\todo{
\begin{itemize}
	\item dimensionality of chemical abundance space
	\item clustering approaches do not take this into account
	\item introduced a model to simultaneously account for the clustering in
		  a lower dimensional latent space
	\item there are issues, though: this is probably  not the right model
	\item identifiability
	\item orthogonal rotation
	\item can interpret these somehow as being astrophysically motivated
	\item at low metallicities how many factors do we prefer, and what can we
		  vaguely interpret these at?
	\item when we do that to large galah data sets, what is the number of factors
	\item clustering in galah
\end{itemize}
}
\section{Conclusions} \label{sec:conclusion}

\acknowledgements
A.~R.~C. is supported in part by Australian Research Council
Discovery Project DP160100637.
% Gaia acknowledgement?
% GALAH acknowledgement?

\software{
	\package{Astropy} \citep{astropy:v1,astropy:v2},
    \package{IPython} \citep{ipython},
    \package{matplotlib} \citep{mpl},
    \package{numpy} \citep{numpy},
    \package{scipy} \citep{scipy},
    \package{Stan} \citep{stan},
    \package{Jupyter Notebooks} \citep{jupyter-notebooks}
}    

\appendix

\section{Nomenclature}\label{app:symbols}

\todo{summarise symbols}

\bibliographystyle{aasjournal}
\bibliography{mcfa}

\end{document}
